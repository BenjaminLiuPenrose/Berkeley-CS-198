{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a framework for creating and training neural networks. It's one of the most common neural network libraries, alongside TensorFlow, and is used extensively in both academia and industry. PyTorch was designed for simplicity -- The code you write is the code that is executed, unlike TensorFlow, and there's very little overhead in terms of reused code. In this homework, we'll explore the basic operations within PyTorch, and we'll design a neural network to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the libraries that we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can't import torch, go to www.pytorch.org and follow the instructions there for downloading PyTorch. You can select CUDA Version as None, as we won't be working with any GPUs on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, data is stored as multidimensional arrays, called tensors. Tensors are very similar to numpy's ndarrays, and they support many of the same operations. We can define tensors by explicity setting the values, using a python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[ 1,  2],\n",
      "        [ 4, -3]])\n",
      "\n",
      "\n",
      "B:\n",
      "tensor([[ 3,  1],\n",
      "        [-2,  3]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, -3]])\n",
    "B = torch.tensor([[3, 1], [-2, 3]])\n",
    "\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"B:\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like numpy, PyTorch supports operations like addition, multiplication, transposition, dot products, and concatenation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of A and B:\n",
      "tensor([[4, 3],\n",
      "        [2, 0]])\n",
      "\n",
      "\n",
      "Elementwise product of A and B:\n",
      "tensor([[ 3,  2],\n",
      "        [-8, -9]])\n",
      "\n",
      "\n",
      "Matrix product of A and B:\n",
      "tensor([[-1,  7],\n",
      "        [18, -5]])\n",
      "\n",
      "\n",
      "Transposition of A:\n",
      "tensor([[ 1,  4],\n",
      "        [ 2, -3]])\n",
      "\n",
      "\n",
      "Concatenation of A and B in the 0th dimension:\n",
      "tensor([[ 1,  2],\n",
      "        [ 4, -3],\n",
      "        [ 3,  1],\n",
      "        [-2,  3]])\n",
      "\n",
      "\n",
      "Concatenation of A and B in the 1st dimension:\n",
      "tensor([[ 1,  2,  3,  1],\n",
      "        [ 4, -3, -2,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of A and B:\")\n",
    "print(torch.add(A, B))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Elementwise product of A and B:\")\n",
    "print(torch.mul(A, B))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Matrix product of A and B:\")\n",
    "print(torch.matmul(A, B))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Transposition of A:\")\n",
    "print(torch.t(A))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Concatenation of A and B in the 0th dimension:\")\n",
    "print(torch.cat((A, B), dim=0))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Concatenation of A and B in the 1st dimension:\")\n",
    "print(torch.cat((A, B), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has tools for creating large tensors automatically, without explicity specifying the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x4x5 Tensor of Zeros:\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "\n",
      "\n",
      "5x5 Tensor with random elements sampled from a standard normal distrubtion:\n",
      "tensor([[-0.1883, -1.3210,  1.4432,  1.0341,  0.2128],\n",
      "        [-1.3903,  1.1469,  0.3570, -1.5392, -0.1707],\n",
      "        [ 0.5931, -0.1459,  0.9271, -0.9253, -0.0581],\n",
      "        [ 0.2566, -2.0578, -1.1957,  0.0302, -0.8827],\n",
      "        [ 0.9766, -0.1762, -0.1773, -0.1481, -0.0236]])\n",
      "\n",
      "\n",
      "Tensor created from a range:\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(\"3x4x5 Tensor of Zeros:\")\n",
    "print(torch.zeros(3, 4, 5))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"5x5 Tensor with random elements sampled from a standard normal distrubtion:\")\n",
    "print(torch.randn(5, 5))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Tensor created from a range:\")\n",
    "print(torch.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use PyTorch tensors to complete the following computation:\n",
    "\n",
    "Create a tensor of integers from the range 0 to 99, inclusive. Add 0.5 to each element in the tensor, and square each element of the result. Then, negate each element of the tensor, and apply the exponential to each element (i.e., change each element x into e^x). Now, sum all the elements of the tensor and print your result.\n",
    "\n",
    "If you're right, you should get something very close to $$\\frac{1}{2} \\cdot \\sqrt{\\pi} \\approx 0.8862 .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.])\n",
      "tensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000])\n",
      "tensor([ 0.2500,  2.2500,  6.2500, 12.2500, 20.2500])\n",
      "tensor([ -0.2500,  -2.2500,  -6.2500, -12.2500, -20.2500])\n",
      "tensor([7.7880e-01, 1.0540e-01, 1.9305e-03, 4.7851e-06, 1.6052e-09])\n",
      "tensor(0.8861)\n"
     ]
    }
   ],
   "source": [
    "val = torch.arange(100).float()\n",
    "\n",
    "### <YOUR CODE HERE> ####\n",
    "print(val[:5])\n",
    "val = val + 0.5;\n",
    "print(val[:5])\n",
    "val = val ** 2;\n",
    "print(val[:5])\n",
    "val = -val;\n",
    "print(val[:5])\n",
    "val = np.exp(val);\n",
    "print(val[:5])\n",
    "val = torch.sum(val);\n",
    "### </YOUR CODE HERE> ###\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, you'll need to use the PyTorch documentation at https://pytorch.org/docs/stable/torch.html. Luckily, PyTorch has very well-written docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd is PyTorch's automatic differentiation tool: It allows us to compute gradients by keeping track of all the operations that have happened to a tensor. In the context of neural networks, we'll interpret these gradient calculations as backpropagating a loss through a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how autograd works, we first need to understand the idea of a __computation graph__. A computation graph is a directed, acyclic graph (DAG) that contains a blueprint of a sequence of operations. For a neural network, these computations consist of matrix multiplications, bias additions, ReLUs, softmaxes, etc. Nodes in this graph consist of the operations themselves, while the edges represent tensors that flow forward along this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the creation of this graph is __dynamic__. This means that tensors themselves keep track of their own computational history, and this history is build as the tensors flow through the network; this is unlike TensorFlow, where an external controller keeps track of the entire computation graph. This dynamic creation of the computation graph allows for lots of cool control-flows that are not possible (or at least very difficult) in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)\n",
    "<center>_Dynamic computation graphs are cool!_</center>\n",
    "_ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a simple computation to see what autograd is doing. First, let's create two tensors and add them together. To signal to PyTorch that we want to build a computation graph, we must set the flag requires_grad to be True when creating a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2], dtype=torch.float, requires_grad=True)\n",
    "b = torch.tensor([8, 3], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since a and b are both part of our computation graph, c will automatically be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add a tensor to our computation graph in this way, our tensor now has a grad_fn attribute. This attribute tells autograd how this tensor was generated, and what tensor(s) this particular node was created from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of c, its grad_fn is of type AddBackward1, PyTorch's notation for a tensor that was created by adding two tensors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ThAddBackward at 0x20bd1b98c88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every grad_fn has an attribute called next_functions: This attribute lets the grad_fn pass on its gradient to the tensors that were used to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x20bd1bf20f0>, 0),\n",
       " (<AccumulateGrad at 0x20bd1bf2b38>, 0))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we extract the tensor values corresponding to each of these functions, we can see a and b! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.], requires_grad=True)\n",
      "tensor([8., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(c.grad_fn.next_functions[0][0].variable)\n",
    "print(c.grad_fn.next_functions[1][0].variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, autograd allows a tensor to record its entire computational history, implicitly creating a computational graph -- All dynamically and on-the-fly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Modules and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, collections of operations are encapsulated as __modules__. One way to visualize a module is to take a section of a computational graph and collapse it into a single node. Not only are modules useful for encapsulation, they have the ability to keep track of tensors that are contained inside of them: To do this, simply wrap a tensor with the class torch.nn.Parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a module, we must subclass the type torch.nn.Module. In addition, we must define a _forward_ method that tells PyTorch how to traverse through a module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's define a logistic regression module. This module will contain two parameters: The weight vector and the bias. Calling the _forward_ method will output a probability between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(10))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, vector):\n",
    "        return self.sigmoid(torch.dot(vector, self.weight) + self.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have fixed the dimension of our weight to be 10, so our module will only accept 10-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a random vector and pass it through the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = LogisticRegression()\n",
    "vector = torch.randn(10)\n",
    "output = module(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0122], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, say that our loss function is mean-squared-error and our target value is 1. We can then write our loss as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (output - 1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9757], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize this loss, we just call loss.backward(), and all the gradients will be computed for us! Note that wrapping a tensor as a Parameter will automatically set requires_grad = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0103, -0.0066, -0.0586,  0.0296, -0.0111,  0.0222, -0.0075,  0.0070,\n",
      "         0.0192, -0.0358])\n",
      "tensor([-0.0239])\n"
     ]
    }
   ],
   "source": [
    "print(module.weight.grad)\n",
    "print(module.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected Networks for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using this knowledge, you will create a neural network in PyTorch for image classification on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "trainset = [(np.asarray(image) / 256, label) for image, label in trainset]\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4096, shuffle=True)\n",
    "\n",
    "val_and_test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "val_and_test_set = [(np.asarray(image) / 256, label) for image, label in val_and_test_set]\n",
    "\n",
    "valset = val_and_test_set[:5000]\n",
    "valset = (\n",
    "    torch.stack([torch.tensor(pair[0]) for pair in valset]),\n",
    "    torch.tensor([pair[1] for pair in valset])\n",
    ")\n",
    "testset = val_and_test_set[5000:]\n",
    "testset = (\n",
    "    torch.stack([torch.tensor(pair[0]) for pair in testset]),\n",
    "    torch.tensor([pair[1] for pair in testset])\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 consists of 32 x 32 color images, each corresponding to a unique class indicating the object present within the image. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in trainset[:4]:\n",
    "    plt.title(classes[label])\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already split the dataset into training, validation, and test sets for you. \n",
    "\n",
    "**Your assignment is to create and train a neural network that properly classifies images in the CIFAR-10 dataset. You should achieve above 40% classification accuracy on the test set in order to receive full credit on this homework.**\n",
    "\n",
    "We've given you some starter code to achieve this task, but the rest is up to you. Google is your friend -- Looking things up on the PyTorch docs and on StackOverflow will be helpful.\n",
    "\n",
    "To turn in the assignment, convert this notebook to a PDF (File -> Download As -> PDF via LaTeX) and submit to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ### <YOUR CODE HERE> ####\n",
    "        self.fc1 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(layer_sizes[1], layer_sizes[2])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(layer_sizes[2], layer_sizes[3])\n",
    "        ### </YOUR CODE HERE> ###\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \n",
    "        ### <YOUR CODE HERE> ####\n",
    "        x = self.fc1(images)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        ### </YOUR CODE HERE> ###\n",
    "\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu3(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu4(x)\n",
    "#         x = self.fc3(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(images):\n",
    "    '''\n",
    "    Reshapes a set of images of the shape (batch_size, width, height, channels)\n",
    "    into the proper shape (batch_size, width * height * channels) that the model can accept.\n",
    "    '''\n",
    "    return images.reshape(images.shape[0], -1).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=3072, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "(epoch, train_loss, val_loss) = (0, 2.2998661444737363, 2.2933926582336426)\n",
      "(epoch, train_loss, val_loss) = (1, 2.2879747977623572, 2.282579183578491)\n",
      "(epoch, train_loss, val_loss) = (2, 2.2778605131002574, 2.273533344268799)\n",
      "(epoch, train_loss, val_loss) = (3, 2.269552395893977, 2.264289379119873)\n",
      "(epoch, train_loss, val_loss) = (4, 2.258379862858699, 2.25252628326416)\n",
      "(epoch, train_loss, val_loss) = (5, 2.245474411891057, 2.238726854324341)\n",
      "(epoch, train_loss, val_loss) = (6, 2.2325083292447605, 2.226984977722168)\n",
      "(epoch, train_loss, val_loss) = (7, 2.221486660150381, 2.2153706550598145)\n",
      "(epoch, train_loss, val_loss) = (8, 2.2089426700885477, 2.20198655128479)\n",
      "(epoch, train_loss, val_loss) = (9, 2.196319103240967, 2.19227933883667)\n",
      "(epoch, train_loss, val_loss) = (10, 2.1869454383850098, 2.180569648742676)\n",
      "(epoch, train_loss, val_loss) = (11, 2.172657159658579, 2.165815830230713)\n",
      "(epoch, train_loss, val_loss) = (12, 2.1608734864455004, 2.1563832759857178)\n",
      "(epoch, train_loss, val_loss) = (13, 2.153778021152203, 2.1480767726898193)\n",
      "(epoch, train_loss, val_loss) = (14, 2.1421107879051795, 2.1340718269348145)\n",
      "(epoch, train_loss, val_loss) = (15, 2.128221530180711, 2.122772455215454)\n",
      "(epoch, train_loss, val_loss) = (16, 2.120535502066979, 2.122997522354126)\n",
      "(epoch, train_loss, val_loss) = (17, 2.120377155450674, 2.121582269668579)\n",
      "(epoch, train_loss, val_loss) = (18, 2.113106342462393, 2.104022979736328)\n",
      "(epoch, train_loss, val_loss) = (19, 2.0950436408703146, 2.08624005317688)\n",
      "(epoch, train_loss, val_loss) = (20, 2.0854589572319617, 2.0886218547821045)\n",
      "(epoch, train_loss, val_loss) = (21, 2.09444207411546, 2.097816228866577)\n",
      "(epoch, train_loss, val_loss) = (22, 2.0973052978515625, 2.090062379837036)\n",
      "(epoch, train_loss, val_loss) = (23, 2.081148092563336, 2.0686450004577637)\n",
      "(epoch, train_loss, val_loss) = (24, 2.0625250339508057, 2.054841995239258)\n",
      "(epoch, train_loss, val_loss) = (25, 2.0572965878706713, 2.064722776412964)\n",
      "(epoch, train_loss, val_loss) = (26, 2.0737997935368466, 2.091365337371826)\n",
      "(epoch, train_loss, val_loss) = (27, 2.0898611545562744, 2.1040890216827393)\n",
      "(epoch, train_loss, val_loss) = (28, 2.0869635251852183, 2.0823137760162354)\n",
      "(epoch, train_loss, val_loss) = (29, 2.0598523249992957, 2.0463762283325195)\n",
      "(epoch, train_loss, val_loss) = (30, 2.03546588237469, 2.0334970951080322)\n",
      "(epoch, train_loss, val_loss) = (31, 2.04006158388578, 2.0533108711242676)\n",
      "(epoch, train_loss, val_loss) = (32, 2.068164898799016, 2.084616184234619)\n",
      "(epoch, train_loss, val_loss) = (33, 2.0944738388061523, 2.0976927280426025)\n",
      "(epoch, train_loss, val_loss) = (34, 2.0941154590019813, 2.0783092975616455)\n",
      "(epoch, train_loss, val_loss) = (35, 2.0632282403799205, 2.041537046432495)\n",
      "(epoch, train_loss, val_loss) = (36, 2.0287398191598744, 2.0184311866760254)\n",
      "(epoch, train_loss, val_loss) = (37, 2.020301176951482, 2.030855178833008)\n",
      "(epoch, train_loss, val_loss) = (38, 2.046702715066763, 2.075608253479004)\n",
      "(epoch, train_loss, val_loss) = (39, 2.0922320989462047, 2.119422197341919)\n",
      "(epoch, train_loss, val_loss) = (40, 2.1133978366851807, 2.1241397857666016)\n",
      "(epoch, train_loss, val_loss) = (41, 2.100649228462806, 2.090725898742676)\n",
      "(epoch, train_loss, val_loss) = (42, 2.067240073130681, 2.063060760498047)\n",
      "(epoch, train_loss, val_loss) = (43, 2.0550776628347545, 2.070990800857544)\n",
      "(epoch, train_loss, val_loss) = (44, 2.0786808270674486, 2.097318172454834)\n",
      "(epoch, train_loss, val_loss) = (45, 2.101692878282987, 2.1105358600616455)\n",
      "(epoch, train_loss, val_loss) = (46, 2.1016543278327355, 2.0991556644439697)\n",
      "(epoch, train_loss, val_loss) = (47, 2.087341913810143, 2.0766725540161133)\n",
      "(epoch, train_loss, val_loss) = (48, 2.069377514032217, 2.065830945968628)\n",
      "(epoch, train_loss, val_loss) = (49, 2.0723246060884914, 2.0834362506866455)\n",
      "(epoch, train_loss, val_loss) = (50, 2.0987769090212307, 2.128109931945801)\n",
      "(epoch, train_loss, val_loss) = (51, 2.1492448770082913, 2.177309036254883)\n",
      "(epoch, train_loss, val_loss) = (52, 2.1835925579071045, 2.199411630630493)\n",
      "(epoch, train_loss, val_loss) = (53, 2.187590580720168, 2.1764309406280518)\n",
      "(epoch, train_loss, val_loss) = (54, 2.1430035554445706, 2.1177000999450684)\n",
      "(epoch, train_loss, val_loss) = (55, 2.078527413881742, 2.0514419078826904)\n",
      "(epoch, train_loss, val_loss) = (56, 2.0204995962289662, 2.003782033920288)\n",
      "(epoch, train_loss, val_loss) = (57, 1.9863695548130915, 1.986330509185791)\n",
      "(epoch, train_loss, val_loss) = (58, 1.9822233731930072, 1.9964066743850708)\n",
      "(epoch, train_loss, val_loss) = (59, 2.001177806120652, 2.0253279209136963)\n",
      "(epoch, train_loss, val_loss) = (60, 2.0324086134250345, 2.060978412628174)\n",
      "(epoch, train_loss, val_loss) = (61, 2.068401941886315, 2.092938184738159)\n",
      "(epoch, train_loss, val_loss) = (62, 2.0920909368074856, 2.1148016452789307)\n",
      "(epoch, train_loss, val_loss) = (63, 2.1130608228536754, 2.12506103515625)\n",
      "(epoch, train_loss, val_loss) = (64, 2.1172967690687914, 2.127082109451294)\n",
      "(epoch, train_loss, val_loss) = (65, 2.122872444299551, 2.125903606414795)\n",
      "(epoch, train_loss, val_loss) = (66, 2.119967827430138, 2.1248340606689453)\n",
      "(epoch, train_loss, val_loss) = (67, 2.1219575221721945, 2.12290620803833)\n",
      "(epoch, train_loss, val_loss) = (68, 2.1180704006781945, 2.1152565479278564)\n",
      "(epoch, train_loss, val_loss) = (69, 2.1057734122643104, 2.097871780395508)\n",
      "(epoch, train_loss, val_loss) = (70, 2.0818010660318227, 2.0714051723480225)\n",
      "(epoch, train_loss, val_loss) = (71, 2.0598596242757945, 2.041952610015869)\n",
      "(epoch, train_loss, val_loss) = (72, 2.0301107259897084, 2.0168213844299316)\n",
      "(epoch, train_loss, val_loss) = (73, 2.007195096749526, 2.0009050369262695)\n",
      "(epoch, train_loss, val_loss) = (74, 1.9940528044333825, 1.994356393814087)\n",
      "(epoch, train_loss, val_loss) = (75, 1.991130544589116, 1.9955097436904907)\n",
      "(epoch, train_loss, val_loss) = (76, 1.9917847560002253, 2.001542329788208)\n",
      "(epoch, train_loss, val_loss) = (77, 2.0018273133497972, 2.0090668201446533)\n",
      "(epoch, train_loss, val_loss) = (78, 2.0079372295966516, 2.0148696899414062)\n",
      "(epoch, train_loss, val_loss) = (79, 2.01193289573376, 2.0168094635009766)\n",
      "(epoch, train_loss, val_loss) = (80, 2.0118210224004893, 2.014183282852173)\n",
      "(epoch, train_loss, val_loss) = (81, 2.0090632346960215, 2.0079400539398193)\n",
      "(epoch, train_loss, val_loss) = (82, 2.002950219007639, 2.000208854675293)\n",
      "(epoch, train_loss, val_loss) = (83, 1.99449518093696, 1.993632435798645)\n",
      "(epoch, train_loss, val_loss) = (84, 1.9909124007591834, 1.9903579950332642)\n",
      "(epoch, train_loss, val_loss) = (85, 1.9894439807304969, 1.9915530681610107)\n",
      "(epoch, train_loss, val_loss) = (86, 1.9930516206301176, 1.9968949556350708)\n",
      "(epoch, train_loss, val_loss) = (87, 1.9987694666935847, 2.004683017730713)\n",
      "(epoch, train_loss, val_loss) = (88, 2.0087745648164015, 2.012718439102173)\n",
      "(epoch, train_loss, val_loss) = (89, 2.0149032519413876, 2.018996000289917)\n",
      "(epoch, train_loss, val_loss) = (90, 2.0199323250697208, 2.021973133087158)\n",
      "(epoch, train_loss, val_loss) = (91, 2.0191858181586633, 2.0211164951324463)\n",
      "(epoch, train_loss, val_loss) = (92, 2.0157920580643873, 2.0168099403381348)\n",
      "(epoch, train_loss, val_loss) = (93, 2.0103168304149923, 2.01027774810791)\n",
      "(epoch, train_loss, val_loss) = (94, 2.0057218624995303, 2.0031397342681885)\n",
      "(epoch, train_loss, val_loss) = (95, 1.9980829128852258, 1.9969329833984375)\n",
      "(epoch, train_loss, val_loss) = (96, 1.9917253989439745, 1.9928452968597412)\n",
      "(epoch, train_loss, val_loss) = (97, 1.9909651921345637, 1.9909402132034302)\n",
      "(epoch, train_loss, val_loss) = (98, 1.9875843158135047, 1.9905301332473755)\n",
      "(epoch, train_loss, val_loss) = (99, 1.9919261198777418, 1.9906299114227295)\n",
      "(epoch, train_loss, val_loss) = (100, 1.9895362120408278, 1.9890069961547852)\n",
      "(epoch, train_loss, val_loss) = (101, 1.9848612730319684, 1.9844298362731934)\n",
      "(epoch, train_loss, val_loss) = (102, 1.9794580844732432, 1.9779093265533447)\n",
      "(epoch, train_loss, val_loss) = (103, 1.9715938568115234, 1.971143364906311)\n",
      "(epoch, train_loss, val_loss) = (104, 1.9649812991802509, 1.9641814231872559)\n",
      "(epoch, train_loss, val_loss) = (105, 1.9583960404762855, 1.9565181732177734)\n",
      "(epoch, train_loss, val_loss) = (106, 1.9502618587934053, 1.947994589805603)\n",
      "(epoch, train_loss, val_loss) = (107, 1.9459372483766997, 1.9401109218597412)\n",
      "(epoch, train_loss, val_loss) = (108, 1.9408016938429613, 1.9351316690444946)\n",
      "(epoch, train_loss, val_loss) = (109, 1.9358912798074575, 1.9337124824523926)\n",
      "(epoch, train_loss, val_loss) = (110, 1.9329265172664936, 1.9351328611373901)\n",
      "(epoch, train_loss, val_loss) = (111, 1.9380520215401282, 1.937766432762146)\n",
      "(epoch, train_loss, val_loss) = (112, 1.940028685789842, 1.939558744430542)\n",
      "(epoch, train_loss, val_loss) = (113, 1.9408039037997906, 1.9393500089645386)\n",
      "(epoch, train_loss, val_loss) = (114, 1.940491272852971, 1.9373250007629395)\n",
      "(epoch, train_loss, val_loss) = (115, 1.9342562785515418, 1.9345800876617432)\n",
      "(epoch, train_loss, val_loss) = (116, 1.9352660270837636, 1.9326541423797607)\n",
      "(epoch, train_loss, val_loss) = (117, 1.9347389936447144, 1.9327003955841064)\n",
      "(epoch, train_loss, val_loss) = (118, 1.9307587972054114, 1.934910535812378)\n",
      "(epoch, train_loss, val_loss) = (119, 1.9355245278431819, 1.938293218612671)\n",
      "(epoch, train_loss, val_loss) = (120, 1.9382494871432965, 1.9413206577301025)\n",
      "(epoch, train_loss, val_loss) = (121, 1.9380371570587158, 1.9421517848968506)\n",
      "(epoch, train_loss, val_loss) = (122, 1.9384863560016339, 1.9398269653320312)\n",
      "(epoch, train_loss, val_loss) = (123, 1.933168154496413, 1.9344507455825806)\n",
      "(epoch, train_loss, val_loss) = (124, 1.926806743328388, 1.9272412061691284)\n",
      "(epoch, train_loss, val_loss) = (125, 1.92187539430765, 1.9200356006622314)\n",
      "(epoch, train_loss, val_loss) = (126, 1.914287988956158, 1.9146982431411743)\n",
      "(epoch, train_loss, val_loss) = (127, 1.9134546793424165, 1.9126636981964111)\n",
      "(epoch, train_loss, val_loss) = (128, 1.9109297440602229, 1.9140821695327759)\n",
      "(epoch, train_loss, val_loss) = (129, 1.917406128003047, 1.91825532913208)\n",
      "(epoch, train_loss, val_loss) = (130, 1.924484344629141, 1.9237974882125854)\n",
      "(epoch, train_loss, val_loss) = (131, 1.9270986593686616, 1.9289159774780273)\n",
      "(epoch, train_loss, val_loss) = (132, 1.9329565580074604, 1.9316555261611938)\n",
      "(epoch, train_loss, val_loss) = (133, 1.935420063825754, 1.9306007623672485)\n",
      "(epoch, train_loss, val_loss) = (134, 1.9359459693615253, 1.925134539604187)\n",
      "(epoch, train_loss, val_loss) = (135, 1.9217190742492676, 1.9161878824234009)\n",
      "(epoch, train_loss, val_loss) = (136, 1.9131888426267183, 1.9055092334747314)\n",
      "(epoch, train_loss, val_loss) = (137, 1.9048406802690947, 1.8955261707305908)\n",
      "(epoch, train_loss, val_loss) = (138, 1.89729538330665, 1.8882805109024048)\n",
      "(epoch, train_loss, val_loss) = (139, 1.8888254532447228, 1.8855209350585938)\n",
      "(epoch, train_loss, val_loss) = (140, 1.8886781289027288, 1.888143539428711)\n",
      "(epoch, train_loss, val_loss) = (141, 1.890951055746812, 1.8957504034042358)\n",
      "(epoch, train_loss, val_loss) = (142, 1.897682290810805, 1.9067076444625854)\n",
      "(epoch, train_loss, val_loss) = (143, 1.9110624423393836, 1.9187482595443726)\n",
      "(epoch, train_loss, val_loss) = (144, 1.9206379743722768, 1.9290767908096313)\n",
      "(epoch, train_loss, val_loss) = (145, 1.928897527547983, 1.9355617761611938)\n",
      "(epoch, train_loss, val_loss) = (146, 1.9307080048781176, 1.9369945526123047)\n",
      "(epoch, train_loss, val_loss) = (147, 1.935370105963487, 1.9333316087722778)\n",
      "(epoch, train_loss, val_loss) = (148, 1.9247557475016668, 1.925511360168457)\n",
      "(epoch, train_loss, val_loss) = (149, 1.9211005155856793, 1.9153801202774048)\n",
      "(epoch, train_loss, val_loss) = (150, 1.908160530603849, 1.9051681756973267)\n",
      "(epoch, train_loss, val_loss) = (151, 1.902877688407898, 1.896796703338623)\n",
      "(epoch, train_loss, val_loss) = (152, 1.896235392643855, 1.8915618658065796)\n",
      "(epoch, train_loss, val_loss) = (153, 1.8915690091940074, 1.8897494077682495)\n",
      "(epoch, train_loss, val_loss) = (154, 1.893176610653217, 1.8910988569259644)\n",
      "(epoch, train_loss, val_loss) = (155, 1.8965192116223848, 1.8945976495742798)\n",
      "(epoch, train_loss, val_loss) = (156, 1.8981899481553297, 1.8992033004760742)\n",
      "(epoch, train_loss, val_loss) = (157, 1.9029186505537767, 1.9038734436035156)\n",
      "(epoch, train_loss, val_loss) = (158, 1.9117572857783391, 1.9077818393707275)\n",
      "(epoch, train_loss, val_loss) = (159, 1.9140082964530358, 1.910194754600525)\n",
      "(epoch, train_loss, val_loss) = (160, 1.9183507882631743, 1.9106234312057495)\n",
      "(epoch, train_loss, val_loss) = (161, 1.9112398899518526, 1.9089257717132568)\n",
      "(epoch, train_loss, val_loss) = (162, 1.9117117661696215, 1.9050195217132568)\n",
      "(epoch, train_loss, val_loss) = (163, 1.907018203001756, 1.8991025686264038)\n",
      "(epoch, train_loss, val_loss) = (164, 1.8990481266608605, 1.891703724861145)\n",
      "(epoch, train_loss, val_loss) = (165, 1.8917801930354192, 1.8836396932601929)\n",
      "(epoch, train_loss, val_loss) = (166, 1.8808810802606435, 1.875878095626831)\n",
      "(epoch, train_loss, val_loss) = (167, 1.8778469837628877, 1.8697441816329956)\n",
      "(epoch, train_loss, val_loss) = (168, 1.8718701417629535, 1.866330862045288)\n",
      "(epoch, train_loss, val_loss) = (169, 1.8707682352799635, 1.8661023378372192)\n",
      "(epoch, train_loss, val_loss) = (170, 1.871070229090177, 1.8691093921661377)\n",
      "(epoch, train_loss, val_loss) = (171, 1.8751771358343272, 1.8751777410507202)\n",
      "(epoch, train_loss, val_loss) = (172, 1.8789373544546275, 1.8834036588668823)\n",
      "(epoch, train_loss, val_loss) = (173, 1.8895352712044349, 1.8924065828323364)\n",
      "(epoch, train_loss, val_loss) = (174, 1.896130736057575, 1.9007009267807007)\n",
      "(epoch, train_loss, val_loss) = (175, 1.9067316422095666, 1.9069421291351318)\n",
      "(epoch, train_loss, val_loss) = (176, 1.906523273541377, 1.9100406169891357)\n",
      "(epoch, train_loss, val_loss) = (177, 1.9124859021260188, 1.9092222452163696)\n",
      "(epoch, train_loss, val_loss) = (178, 1.9070912141066332, 1.9043781757354736)\n",
      "(epoch, train_loss, val_loss) = (179, 1.9016984701156616, 1.8961129188537598)\n",
      "(epoch, train_loss, val_loss) = (180, 1.8910412788391113, 1.8855726718902588)\n",
      "(epoch, train_loss, val_loss) = (181, 1.8838949111791758, 1.8741809129714966)\n",
      "(epoch, train_loss, val_loss) = (182, 1.8715156408456655, 1.8634119033813477)\n",
      "(epoch, train_loss, val_loss) = (183, 1.8652701652967012, 1.8545478582382202)\n",
      "(epoch, train_loss, val_loss) = (184, 1.8583582731393666, 1.848540186882019)\n",
      "(epoch, train_loss, val_loss) = (185, 1.8553594534213727, 1.8457872867584229)\n",
      "(epoch, train_loss, val_loss) = (186, 1.854025437281682, 1.846083164215088)\n",
      "(epoch, train_loss, val_loss) = (187, 1.8534832917726958, 1.8490021228790283)\n",
      "(epoch, train_loss, val_loss) = (188, 1.8605449749873235, 1.8536866903305054)\n",
      "(epoch, train_loss, val_loss) = (189, 1.8678714495438795, 1.8593642711639404)\n",
      "(epoch, train_loss, val_loss) = (190, 1.8735957879286547, 1.8652050495147705)\n",
      "(epoch, train_loss, val_loss) = (191, 1.8773191983883197, 1.8703995943069458)\n",
      "(epoch, train_loss, val_loss) = (192, 1.8822295023844793, 1.874522089958191)\n",
      "(epoch, train_loss, val_loss) = (193, 1.884116035241347, 1.877197504043579)\n",
      "(epoch, train_loss, val_loss) = (194, 1.8870108677790716, 1.8781400918960571)\n",
      "(epoch, train_loss, val_loss) = (195, 1.886395500256465, 1.8772300481796265)\n",
      "(epoch, train_loss, val_loss) = (196, 1.881711143713731, 1.8744548559188843)\n",
      "(epoch, train_loss, val_loss) = (197, 1.877705427316519, 1.8701285123825073)\n",
      "(epoch, train_loss, val_loss) = (198, 1.8784910073647132, 1.8647048473358154)\n",
      "(epoch, train_loss, val_loss) = (199, 1.8709988502355723, 1.8586043119430542)\n",
      "(epoch, train_loss, val_loss) = (200, 1.8652783907376802, 1.8525478839874268)\n",
      "(epoch, train_loss, val_loss) = (201, 1.8563724206044123, 1.84720778465271)\n",
      "(epoch, train_loss, val_loss) = (202, 1.8536059581316435, 1.8430683612823486)\n",
      "(epoch, train_loss, val_loss) = (203, 1.8528103828430176, 1.8406299352645874)\n",
      "(epoch, train_loss, val_loss) = (204, 1.8526350534879243, 1.8399574756622314)\n",
      "(epoch, train_loss, val_loss) = (205, 1.8517574622080877, 1.8411095142364502)\n",
      "(epoch, train_loss, val_loss) = (206, 1.8510935031450713, 1.84397554397583)\n",
      "(epoch, train_loss, val_loss) = (207, 1.8572388428908129, 1.8481886386871338)\n",
      "(epoch, train_loss, val_loss) = (208, 1.863966886813824, 1.8532825708389282)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch, train_loss, val_loss) = (209, 1.8666332226533155, 1.8585929870605469)\n",
      "(epoch, train_loss, val_loss) = (210, 1.8720906697786772, 1.8633781671524048)\n",
      "(epoch, train_loss, val_loss) = (211, 1.8762030876599824, 1.8670531511306763)\n",
      "(epoch, train_loss, val_loss) = (212, 1.8778589230317335, 1.8691306114196777)\n",
      "(epoch, train_loss, val_loss) = (213, 1.8818543965999897, 1.8691956996917725)\n",
      "(epoch, train_loss, val_loss) = (214, 1.8798141112694373, 1.8670014142990112)\n",
      "(epoch, train_loss, val_loss) = (215, 1.8760960102081299, 1.8626497983932495)\n",
      "(epoch, train_loss, val_loss) = (216, 1.8722810653539805, 1.856441617012024)\n",
      "(epoch, train_loss, val_loss) = (217, 1.8666752210030189, 1.848785161972046)\n",
      "(epoch, train_loss, val_loss) = (218, 1.8632259827393751, 1.840277910232544)\n",
      "(epoch, train_loss, val_loss) = (219, 1.851697637484624, 1.8316307067871094)\n",
      "(epoch, train_loss, val_loss) = (220, 1.8418896931868334, 1.8235588073730469)\n",
      "(epoch, train_loss, val_loss) = (221, 1.8394378240291889, 1.8164814710617065)\n",
      "(epoch, train_loss, val_loss) = (222, 1.8319309124579797, 1.8108652830123901)\n",
      "(epoch, train_loss, val_loss) = (223, 1.8235989625637348, 1.8070218563079834)\n",
      "(epoch, train_loss, val_loss) = (224, 1.8278593191733727, 1.8050130605697632)\n",
      "(epoch, train_loss, val_loss) = (225, 1.8198680144089918, 1.8048183917999268)\n",
      "(epoch, train_loss, val_loss) = (226, 1.8213940858840942, 1.806444764137268)\n",
      "(epoch, train_loss, val_loss) = (227, 1.824073076248169, 1.8095715045928955)\n",
      "(epoch, train_loss, val_loss) = (228, 1.8283601907583384, 1.8137552738189697)\n",
      "(epoch, train_loss, val_loss) = (229, 1.831289218022273, 1.8185147047042847)\n",
      "(epoch, train_loss, val_loss) = (230, 1.8365394060428326, 1.823307991027832)\n",
      "(epoch, train_loss, val_loss) = (231, 1.838500637274522, 1.8276023864746094)\n",
      "(epoch, train_loss, val_loss) = (232, 1.8429578175911536, 1.8308613300323486)\n",
      "(epoch, train_loss, val_loss) = (233, 1.8427430666410005, 1.8325250148773193)\n",
      "(epoch, train_loss, val_loss) = (234, 1.8440330762129564, 1.8323038816452026)\n",
      "(epoch, train_loss, val_loss) = (235, 1.8450163144331713, 1.8301805257797241)\n",
      "(epoch, train_loss, val_loss) = (236, 1.8427925018163829, 1.8263599872589111)\n",
      "(epoch, train_loss, val_loss) = (237, 1.8420469027299147, 1.8211700916290283)\n",
      "(epoch, train_loss, val_loss) = (238, 1.834913317973797, 1.815173625946045)\n",
      "(epoch, train_loss, val_loss) = (239, 1.8263554573059082, 1.8090144395828247)\n",
      "(epoch, train_loss, val_loss) = (240, 1.822922367316026, 1.8033088445663452)\n",
      "(epoch, train_loss, val_loss) = (241, 1.8189059587625356, 1.7986998558044434)\n",
      "(epoch, train_loss, val_loss) = (242, 1.8132802248001099, 1.795330286026001)\n",
      "(epoch, train_loss, val_loss) = (243, 1.8114860149530263, 1.7934277057647705)\n",
      "(epoch, train_loss, val_loss) = (244, 1.8106126143382146, 1.7929060459136963)\n",
      "(epoch, train_loss, val_loss) = (245, 1.8125545611748328, 1.7935020923614502)\n",
      "(epoch, train_loss, val_loss) = (246, 1.8152549450214093, 1.7949906587600708)\n",
      "(epoch, train_loss, val_loss) = (247, 1.8146304075534527, 1.7969845533370972)\n",
      "(epoch, train_loss, val_loss) = (248, 1.8173011816464937, 1.799470067024231)\n",
      "(epoch, train_loss, val_loss) = (249, 1.815971429531391, 1.8022443056106567)\n",
      "(epoch, train_loss, val_loss) = (250, 1.823318178837116, 1.8052287101745605)\n",
      "(epoch, train_loss, val_loss) = (251, 1.8215888280134935, 1.8082177639007568)\n",
      "(epoch, train_loss, val_loss) = (252, 1.8213985883272612, 1.8110679388046265)\n",
      "(epoch, train_loss, val_loss) = (253, 1.8266577170445368, 1.8135827779769897)\n",
      "(epoch, train_loss, val_loss) = (254, 1.824692882024325, 1.8155921697616577)\n",
      "(epoch, train_loss, val_loss) = (255, 1.827128685437716, 1.816790223121643)\n",
      "(epoch, train_loss, val_loss) = (256, 1.8281574157568126, 1.8170208930969238)\n",
      "(epoch, train_loss, val_loss) = (257, 1.8287198910346398, 1.8161990642547607)\n",
      "(epoch, train_loss, val_loss) = (258, 1.8245236965326161, 1.8142156600952148)\n",
      "(epoch, train_loss, val_loss) = (259, 1.8268754940766554, 1.8110246658325195)\n",
      "(epoch, train_loss, val_loss) = (260, 1.8213738203048706, 1.8067556619644165)\n",
      "(epoch, train_loss, val_loss) = (261, 1.816132169503432, 1.8017168045043945)\n",
      "(epoch, train_loss, val_loss) = (262, 1.8110816937226515, 1.796202301979065)\n",
      "(epoch, train_loss, val_loss) = (263, 1.8053337794083815, 1.7906436920166016)\n",
      "(epoch, train_loss, val_loss) = (264, 1.8053530087837806, 1.7855335474014282)\n",
      "(epoch, train_loss, val_loss) = (265, 1.7987801203360925, 1.7812707424163818)\n",
      "(epoch, train_loss, val_loss) = (266, 1.7987494927186232, 1.7783384323120117)\n",
      "(epoch, train_loss, val_loss) = (267, 1.7946301790384145, 1.7769955396652222)\n",
      "(epoch, train_loss, val_loss) = (268, 1.7909643191557665, 1.7772173881530762)\n",
      "(epoch, train_loss, val_loss) = (269, 1.795772754229032, 1.7789385318756104)\n",
      "(epoch, train_loss, val_loss) = (270, 1.8025643733831553, 1.7820918560028076)\n",
      "(epoch, train_loss, val_loss) = (271, 1.802402138710022, 1.7861870527267456)\n",
      "(epoch, train_loss, val_loss) = (272, 1.8083581649340117, 1.7908613681793213)\n",
      "(epoch, train_loss, val_loss) = (273, 1.812364569077125, 1.7955725193023682)\n",
      "(epoch, train_loss, val_loss) = (274, 1.8168847560882568, 1.7997660636901855)\n",
      "(epoch, train_loss, val_loss) = (275, 1.820995385830219, 1.802910327911377)\n",
      "(epoch, train_loss, val_loss) = (276, 1.8240134349236121, 1.8046789169311523)\n",
      "(epoch, train_loss, val_loss) = (277, 1.8280463860585139, 1.8048796653747559)\n",
      "(epoch, train_loss, val_loss) = (278, 1.8215395212173462, 1.8034242391586304)\n",
      "(epoch, train_loss, val_loss) = (279, 1.8247902668439424, 1.8004554510116577)\n",
      "(epoch, train_loss, val_loss) = (280, 1.8207921248215895, 1.7962769269943237)\n",
      "(epoch, train_loss, val_loss) = (281, 1.8132152190575233, 1.791357398033142)\n",
      "(epoch, train_loss, val_loss) = (282, 1.8103361129760742, 1.7861355543136597)\n",
      "(epoch, train_loss, val_loss) = (283, 1.8024041744378896, 1.7810640335083008)\n",
      "(epoch, train_loss, val_loss) = (284, 1.798417513187115, 1.7765947580337524)\n",
      "(epoch, train_loss, val_loss) = (285, 1.795511474976173, 1.773088812828064)\n",
      "(epoch, train_loss, val_loss) = (286, 1.7894500494003296, 1.7706568241119385)\n",
      "(epoch, train_loss, val_loss) = (287, 1.7871087514437163, 1.7692900896072388)\n",
      "(epoch, train_loss, val_loss) = (288, 1.7849226731520433, 1.7687807083129883)\n",
      "(epoch, train_loss, val_loss) = (289, 1.785187840461731, 1.7689203023910522)\n",
      "(epoch, train_loss, val_loss) = (290, 1.7837739174182599, 1.7695232629776)\n",
      "(epoch, train_loss, val_loss) = (291, 1.7808903272335346, 1.770371437072754)\n",
      "(epoch, train_loss, val_loss) = (292, 1.7851488498541026, 1.771344542503357)\n",
      "(epoch, train_loss, val_loss) = (293, 1.7847599249619703, 1.7723220586776733)\n",
      "(epoch, train_loss, val_loss) = (294, 1.789274655855619, 1.7732744216918945)\n",
      "(epoch, train_loss, val_loss) = (295, 1.7852876644868116, 1.7742072343826294)\n",
      "(epoch, train_loss, val_loss) = (296, 1.789180773955125, 1.7751489877700806)\n",
      "(epoch, train_loss, val_loss) = (297, 1.7905290401898897, 1.7760558128356934)\n",
      "(epoch, train_loss, val_loss) = (298, 1.789826173048753, 1.776963710784912)\n",
      "(epoch, train_loss, val_loss) = (299, 1.7896555203657885, 1.7778115272521973)\n",
      "(epoch, train_loss, val_loss) = (300, 1.7936682609411387, 1.7785996198654175)\n",
      "(epoch, train_loss, val_loss) = (301, 1.7897032591012807, 1.7792317867279053)\n",
      "(epoch, train_loss, val_loss) = (302, 1.7964837826215303, 1.7797185182571411)\n",
      "(epoch, train_loss, val_loss) = (303, 1.7968322863945594, 1.7799882888793945)\n",
      "(epoch, train_loss, val_loss) = (304, 1.7951699587015004, 1.7799726724624634)\n",
      "(epoch, train_loss, val_loss) = (305, 1.7978510856628418, 1.7795802354812622)\n",
      "(epoch, train_loss, val_loss) = (306, 1.7954406004685621, 1.7788212299346924)\n",
      "(epoch, train_loss, val_loss) = (307, 1.7945909683520977, 1.7776576280593872)\n",
      "(epoch, train_loss, val_loss) = (308, 1.7956757270372832, 1.7760083675384521)\n",
      "(epoch, train_loss, val_loss) = (309, 1.7974668741226196, 1.7739050388336182)\n",
      "(epoch, train_loss, val_loss) = (310, 1.7903301807550283, 1.7714896202087402)\n",
      "(epoch, train_loss, val_loss) = (311, 1.7900842336507945, 1.7688312530517578)\n",
      "(epoch, train_loss, val_loss) = (312, 1.7852721856190608, 1.7660741806030273)\n",
      "(epoch, train_loss, val_loss) = (313, 1.7834584712982178, 1.7633912563323975)\n",
      "(epoch, train_loss, val_loss) = (314, 1.782728672027588, 1.7609750032424927)\n",
      "(epoch, train_loss, val_loss) = (315, 1.7776449276850774, 1.758971929550171)\n",
      "(epoch, train_loss, val_loss) = (316, 1.777350930067209, 1.7575643062591553)\n",
      "(epoch, train_loss, val_loss) = (317, 1.7767806236560528, 1.7568625211715698)\n",
      "(epoch, train_loss, val_loss) = (318, 1.7765948589031513, 1.7567638158798218)\n",
      "(epoch, train_loss, val_loss) = (319, 1.7763615754934459, 1.7572072744369507)\n",
      "(epoch, train_loss, val_loss) = (320, 1.7777129686795747, 1.7578953504562378)\n",
      "(epoch, train_loss, val_loss) = (321, 1.7775583634009728, 1.7587223052978516)\n",
      "(epoch, train_loss, val_loss) = (322, 1.7766297780550444, 1.7594504356384277)\n",
      "(epoch, train_loss, val_loss) = (323, 1.7773169829295232, 1.7599538564682007)\n",
      "(epoch, train_loss, val_loss) = (324, 1.7784606768534734, 1.7599678039550781)\n",
      "(epoch, train_loss, val_loss) = (325, 1.77884042263031, 1.7592647075653076)\n",
      "(epoch, train_loss, val_loss) = (326, 1.7752324801224928, 1.7578349113464355)\n",
      "(epoch, train_loss, val_loss) = (327, 1.773706344457773, 1.755789875984192)\n",
      "(epoch, train_loss, val_loss) = (328, 1.7745263209709754, 1.7533375024795532)\n",
      "(epoch, train_loss, val_loss) = (329, 1.7727917616183941, 1.7508394718170166)\n",
      "(epoch, train_loss, val_loss) = (330, 1.7706433626321645, 1.7485765218734741)\n",
      "(epoch, train_loss, val_loss) = (331, 1.7661636425898626, 1.7468204498291016)\n",
      "(epoch, train_loss, val_loss) = (332, 1.769870666357187, 1.7457935810089111)\n",
      "(epoch, train_loss, val_loss) = (333, 1.7658426945026104, 1.7455474138259888)\n",
      "(epoch, train_loss, val_loss) = (334, 1.7687462659982534, 1.745835542678833)\n",
      "(epoch, train_loss, val_loss) = (335, 1.7689317923325758, 1.746463418006897)\n",
      "(epoch, train_loss, val_loss) = (336, 1.7737779158812303, 1.7471199035644531)\n",
      "(epoch, train_loss, val_loss) = (337, 1.770413297873277, 1.7474710941314697)\n",
      "(epoch, train_loss, val_loss) = (338, 1.774595783307002, 1.747290015220642)\n",
      "(epoch, train_loss, val_loss) = (339, 1.7733525587962224, 1.746357798576355)\n",
      "(epoch, train_loss, val_loss) = (340, 1.7673147366597102, 1.7446750402450562)\n",
      "(epoch, train_loss, val_loss) = (341, 1.7692426901597242, 1.742340087890625)\n",
      "(epoch, train_loss, val_loss) = (342, 1.7683422106962938, 1.7396901845932007)\n",
      "(epoch, train_loss, val_loss) = (343, 1.7636496378825262, 1.7370513677597046)\n",
      "(epoch, train_loss, val_loss) = (344, 1.7639517050523024, 1.734859824180603)\n",
      "(epoch, train_loss, val_loss) = (345, 1.760427135687608, 1.7333757877349854)\n",
      "(epoch, train_loss, val_loss) = (346, 1.7610432184659517, 1.7328886985778809)\n",
      "(epoch, train_loss, val_loss) = (347, 1.758708302791302, 1.733496904373169)\n",
      "(epoch, train_loss, val_loss) = (348, 1.7562435223506048, 1.7351778745651245)\n",
      "(epoch, train_loss, val_loss) = (349, 1.7604253750581007, 1.7377867698669434)\n",
      "(epoch, train_loss, val_loss) = (350, 1.7626763307131255, 1.7411439418792725)\n",
      "(epoch, train_loss, val_loss) = (351, 1.762765095784114, 1.7448105812072754)\n",
      "(epoch, train_loss, val_loss) = (352, 1.7738607571675227, 1.7483892440795898)\n",
      "(epoch, train_loss, val_loss) = (353, 1.773142851316012, 1.7514599561691284)\n",
      "(epoch, train_loss, val_loss) = (354, 1.7735889233075655, 1.7537001371383667)\n",
      "(epoch, train_loss, val_loss) = (355, 1.7777499052194448, 1.7548717260360718)\n",
      "(epoch, train_loss, val_loss) = (356, 1.7784430063687837, 1.7548136711120605)\n",
      "(epoch, train_loss, val_loss) = (357, 1.7726375323075514, 1.7535113096237183)\n",
      "(epoch, train_loss, val_loss) = (358, 1.7766862374085646, 1.7510603666305542)\n",
      "(epoch, train_loss, val_loss) = (359, 1.774059561582712, 1.7476634979248047)\n",
      "(epoch, train_loss, val_loss) = (360, 1.7686244891240046, 1.7435951232910156)\n",
      "(epoch, train_loss, val_loss) = (361, 1.766112676033607, 1.7392199039459229)\n",
      "(epoch, train_loss, val_loss) = (362, 1.7635866128481352, 1.7348636388778687)\n",
      "(epoch, train_loss, val_loss) = (363, 1.7582103655888484, 1.7308648824691772)\n",
      "(epoch, train_loss, val_loss) = (364, 1.7529379221109243, 1.7273879051208496)\n",
      "(epoch, train_loss, val_loss) = (365, 1.7498177473361676, 1.724488615989685)\n",
      "(epoch, train_loss, val_loss) = (366, 1.7485276735745943, 1.7222402095794678)\n",
      "(epoch, train_loss, val_loss) = (367, 1.7475714866931622, 1.7205005884170532)\n",
      "(epoch, train_loss, val_loss) = (368, 1.7432065560267522, 1.7191320657730103)\n",
      "(epoch, train_loss, val_loss) = (369, 1.7439661851296058, 1.7181204557418823)\n",
      "(epoch, train_loss, val_loss) = (370, 1.7440591958852916, 1.7174701690673828)\n",
      "(epoch, train_loss, val_loss) = (371, 1.7442330855589647, 1.7170627117156982)\n",
      "(epoch, train_loss, val_loss) = (372, 1.7412172372524555, 1.7169201374053955)\n",
      "(epoch, train_loss, val_loss) = (373, 1.7427189350128174, 1.7170294523239136)\n",
      "(epoch, train_loss, val_loss) = (374, 1.742494794038626, 1.7174718379974365)\n",
      "(epoch, train_loss, val_loss) = (375, 1.7444963271801288, 1.7183253765106201)\n",
      "(epoch, train_loss, val_loss) = (376, 1.7431003680595984, 1.7196577787399292)\n",
      "(epoch, train_loss, val_loss) = (377, 1.7429593801498413, 1.7215179204940796)\n",
      "(epoch, train_loss, val_loss) = (378, 1.7427159456106334, 1.7238736152648926)\n",
      "(epoch, train_loss, val_loss) = (379, 1.747789006966811, 1.7266252040863037)\n",
      "(epoch, train_loss, val_loss) = (380, 1.7491210240584154, 1.7295488119125366)\n",
      "(epoch, train_loss, val_loss) = (381, 1.7518266714536226, 1.7324384450912476)\n",
      "(epoch, train_loss, val_loss) = (382, 1.7532923221588135, 1.7349250316619873)\n",
      "(epoch, train_loss, val_loss) = (383, 1.7564707627663245, 1.7368313074111938)\n",
      "(epoch, train_loss, val_loss) = (384, 1.757759112578172, 1.7378984689712524)\n",
      "(epoch, train_loss, val_loss) = (385, 1.761554873906649, 1.737916350364685)\n",
      "(epoch, train_loss, val_loss) = (386, 1.7553675358112042, 1.736777901649475)\n",
      "(epoch, train_loss, val_loss) = (387, 1.756056051987868, 1.7345181703567505)\n",
      "(epoch, train_loss, val_loss) = (388, 1.7554311844018788, 1.7313082218170166)\n",
      "(epoch, train_loss, val_loss) = (389, 1.7512197677905743, 1.7274280786514282)\n",
      "(epoch, train_loss, val_loss) = (390, 1.7489832547994761, 1.7232036590576172)\n",
      "(epoch, train_loss, val_loss) = (391, 1.7432640057343702, 1.71903657913208)\n",
      "(epoch, train_loss, val_loss) = (392, 1.7388078341117272, 1.715181589126587)\n",
      "(epoch, train_loss, val_loss) = (393, 1.737291620327876, 1.7119767665863037)\n",
      "(epoch, train_loss, val_loss) = (394, 1.7346891531577477, 1.7096529006958008)\n",
      "(epoch, train_loss, val_loss) = (395, 1.7337340758397028, 1.708237886428833)\n",
      "(epoch, train_loss, val_loss) = (396, 1.7316669317392201, 1.7076990604400635)\n",
      "(epoch, train_loss, val_loss) = (397, 1.7316634838397686, 1.707863450050354)\n",
      "(epoch, train_loss, val_loss) = (398, 1.7272448448034434, 1.7084447145462036)\n",
      "(epoch, train_loss, val_loss) = (399, 1.7325435143250685, 1.7092111110687256)\n",
      "(epoch, train_loss, val_loss) = (400, 1.7373982117726252, 1.7098679542541504)\n",
      "(epoch, train_loss, val_loss) = (401, 1.7357081266549916, 1.7102277278900146)\n",
      "(epoch, train_loss, val_loss) = (402, 1.7350953817367554, 1.710218906402588)\n",
      "(epoch, train_loss, val_loss) = (403, 1.7357486119637122, 1.7099171876907349)\n",
      "(epoch, train_loss, val_loss) = (404, 1.7352293729782104, 1.7093439102172852)\n",
      "(epoch, train_loss, val_loss) = (405, 1.7390921390973604, 1.7086888551712036)\n",
      "(epoch, train_loss, val_loss) = (406, 1.7315402764540453, 1.7081515789031982)\n",
      "(epoch, train_loss, val_loss) = (407, 1.7295034573628352, 1.7078872919082642)\n",
      "(epoch, train_loss, val_loss) = (408, 1.729746515934284, 1.7079099416732788)\n",
      "(epoch, train_loss, val_loss) = (409, 1.731120778964116, 1.7083024978637695)\n",
      "(epoch, train_loss, val_loss) = (410, 1.72853749531966, 1.709014892578125)\n",
      "(epoch, train_loss, val_loss) = (411, 1.7279895085554857, 1.7099826335906982)\n",
      "(epoch, train_loss, val_loss) = (412, 1.733444122167734, 1.7111479043960571)\n",
      "(epoch, train_loss, val_loss) = (413, 1.7348393201828003, 1.712278127670288)\n",
      "(epoch, train_loss, val_loss) = (414, 1.7377943717516386, 1.713239312171936)\n",
      "(epoch, train_loss, val_loss) = (415, 1.7377460094598622, 1.7139415740966797)\n",
      "(epoch, train_loss, val_loss) = (416, 1.7349031063226552, 1.7142152786254883)\n",
      "(epoch, train_loss, val_loss) = (417, 1.73325672516456, 1.7139441967010498)\n",
      "(epoch, train_loss, val_loss) = (418, 1.7348468211980967, 1.7130993604660034)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch, train_loss, val_loss) = (419, 1.7361999108241155, 1.7116793394088745)\n",
      "(epoch, train_loss, val_loss) = (420, 1.734614528142489, 1.709713101387024)\n",
      "(epoch, train_loss, val_loss) = (421, 1.7371863768650935, 1.7072821855545044)\n",
      "(epoch, train_loss, val_loss) = (422, 1.7316844921845655, 1.704513669013977)\n",
      "(epoch, train_loss, val_loss) = (423, 1.7331329492422252, 1.701586127281189)\n",
      "(epoch, train_loss, val_loss) = (424, 1.7276689822857196, 1.698716640472412)\n",
      "(epoch, train_loss, val_loss) = (425, 1.7255801145847027, 1.696097493171692)\n",
      "(epoch, train_loss, val_loss) = (426, 1.7210661814762995, 1.6938880681991577)\n",
      "(epoch, train_loss, val_loss) = (427, 1.7205297488432665, 1.6922626495361328)\n",
      "(epoch, train_loss, val_loss) = (428, 1.7188650369644165, 1.6913195848464966)\n",
      "(epoch, train_loss, val_loss) = (429, 1.7222113059117243, 1.6911624670028687)\n",
      "(epoch, train_loss, val_loss) = (430, 1.7248931114490216, 1.6917468309402466)\n",
      "(epoch, train_loss, val_loss) = (431, 1.72379980637477, 1.692975401878357)\n",
      "(epoch, train_loss, val_loss) = (432, 1.7244394284028273, 1.6946916580200195)\n",
      "(epoch, train_loss, val_loss) = (433, 1.7285878016398504, 1.6966772079467773)\n",
      "(epoch, train_loss, val_loss) = (434, 1.734165806036729, 1.698710560798645)\n",
      "(epoch, train_loss, val_loss) = (435, 1.7308255250637348, 1.7004872560501099)\n",
      "(epoch, train_loss, val_loss) = (436, 1.7358203942959125, 1.701725959777832)\n",
      "(epoch, train_loss, val_loss) = (437, 1.738641537152804, 1.702207088470459)\n",
      "(epoch, train_loss, val_loss) = (438, 1.7404477871381319, 1.7018060684204102)\n",
      "(epoch, train_loss, val_loss) = (439, 1.7375940909752479, 1.7004801034927368)\n",
      "(epoch, train_loss, val_loss) = (440, 1.7391753655213575, 1.698335886001587)\n",
      "(epoch, train_loss, val_loss) = (441, 1.7341009011635413, 1.6955829858779907)\n",
      "(epoch, train_loss, val_loss) = (442, 1.728937607545119, 1.6925302743911743)\n",
      "(epoch, train_loss, val_loss) = (443, 1.7261278262505164, 1.689448595046997)\n",
      "(epoch, train_loss, val_loss) = (444, 1.7181456364118135, 1.6866543292999268)\n",
      "(epoch, train_loss, val_loss) = (445, 1.7197038118655865, 1.6843812465667725)\n",
      "(epoch, train_loss, val_loss) = (446, 1.7192308352543757, 1.682719111442566)\n",
      "(epoch, train_loss, val_loss) = (447, 1.7146091919678907, 1.681774377822876)\n",
      "(epoch, train_loss, val_loss) = (448, 1.7146608737798839, 1.6815298795700073)\n",
      "(epoch, train_loss, val_loss) = (449, 1.711771350640517, 1.6819250583648682)\n",
      "(epoch, train_loss, val_loss) = (450, 1.7117561743809626, 1.6828007698059082)\n",
      "(epoch, train_loss, val_loss) = (451, 1.7117620431459868, 1.6840193271636963)\n",
      "(epoch, train_loss, val_loss) = (452, 1.7123409784757173, 1.6853785514831543)\n",
      "(epoch, train_loss, val_loss) = (453, 1.7123399055921114, 1.6867780685424805)\n",
      "(epoch, train_loss, val_loss) = (454, 1.7128597314541156, 1.6880403757095337)\n",
      "(epoch, train_loss, val_loss) = (455, 1.7138677835464478, 1.6891392469406128)\n",
      "(epoch, train_loss, val_loss) = (456, 1.7123427666150606, 1.6899218559265137)\n",
      "(epoch, train_loss, val_loss) = (457, 1.7204294388110821, 1.6904064416885376)\n",
      "(epoch, train_loss, val_loss) = (458, 1.7149268480447621, 1.6906239986419678)\n",
      "(epoch, train_loss, val_loss) = (459, 1.713883482492887, 1.6906324625015259)\n",
      "(epoch, train_loss, val_loss) = (460, 1.7150634710605328, 1.6905453205108643)\n",
      "(epoch, train_loss, val_loss) = (461, 1.7178940314512987, 1.6903975009918213)\n",
      "(epoch, train_loss, val_loss) = (462, 1.7120558115152211, 1.6902679204940796)\n",
      "(epoch, train_loss, val_loss) = (463, 1.712078598829416, 1.6902436017990112)\n",
      "(epoch, train_loss, val_loss) = (464, 1.7133437211696918, 1.6903867721557617)\n",
      "(epoch, train_loss, val_loss) = (465, 1.713571236683772, 1.690731406211853)\n",
      "(epoch, train_loss, val_loss) = (466, 1.7163200195019062, 1.6913566589355469)\n",
      "(epoch, train_loss, val_loss) = (467, 1.7132599170391376, 1.6922334432601929)\n",
      "(epoch, train_loss, val_loss) = (468, 1.7175619877301729, 1.6933056116104126)\n",
      "(epoch, train_loss, val_loss) = (469, 1.7186249311153705, 1.694462537765503)\n",
      "(epoch, train_loss, val_loss) = (470, 1.7210681255047138, 1.6956255435943604)\n",
      "(epoch, train_loss, val_loss) = (471, 1.7208280196556678, 1.696599006652832)\n",
      "(epoch, train_loss, val_loss) = (472, 1.7221942314734826, 1.6972845792770386)\n",
      "(epoch, train_loss, val_loss) = (473, 1.7235470368311956, 1.6975412368774414)\n",
      "(epoch, train_loss, val_loss) = (474, 1.724948525428772, 1.697245478630066)\n",
      "(epoch, train_loss, val_loss) = (475, 1.7245044341454139, 1.6962976455688477)\n",
      "(epoch, train_loss, val_loss) = (476, 1.7208128617360041, 1.6946544647216797)\n",
      "(epoch, train_loss, val_loss) = (477, 1.7193530522860014, 1.692353367805481)\n",
      "(epoch, train_loss, val_loss) = (478, 1.7191248765358558, 1.6895033121109009)\n",
      "(epoch, train_loss, val_loss) = (479, 1.713270379946782, 1.6862239837646484)\n",
      "(epoch, train_loss, val_loss) = (480, 1.711492391733023, 1.6827025413513184)\n",
      "(epoch, train_loss, val_loss) = (481, 1.707814592581529, 1.6791741847991943)\n",
      "(epoch, train_loss, val_loss) = (482, 1.7006320861669688, 1.6758760213851929)\n",
      "(epoch, train_loss, val_loss) = (483, 1.6995436961834247, 1.6731241941452026)\n",
      "(epoch, train_loss, val_loss) = (484, 1.6973558756021352, 1.6710357666015625)\n",
      "(epoch, train_loss, val_loss) = (485, 1.6953849975879376, 1.6698251962661743)\n",
      "(epoch, train_loss, val_loss) = (486, 1.6931442664219782, 1.6695820093154907)\n",
      "(epoch, train_loss, val_loss) = (487, 1.6947696575751672, 1.670276165008545)\n",
      "(epoch, train_loss, val_loss) = (488, 1.697329567028926, 1.671866774559021)\n",
      "(epoch, train_loss, val_loss) = (489, 1.699414849281311, 1.6742193698883057)\n",
      "(epoch, train_loss, val_loss) = (490, 1.7023252432162945, 1.677053689956665)\n",
      "(epoch, train_loss, val_loss) = (491, 1.7034069574796236, 1.68007230758667)\n",
      "(epoch, train_loss, val_loss) = (492, 1.709946219737713, 1.682890772819519)\n",
      "(epoch, train_loss, val_loss) = (493, 1.7076673874488244, 1.6851998567581177)\n",
      "(epoch, train_loss, val_loss) = (494, 1.7112278204697828, 1.6867343187332153)\n",
      "(epoch, train_loss, val_loss) = (495, 1.7111184230217567, 1.6872855424880981)\n",
      "(epoch, train_loss, val_loss) = (496, 1.7147078972596388, 1.6867916584014893)\n",
      "(epoch, train_loss, val_loss) = (497, 1.714504627081064, 1.685241937637329)\n",
      "(epoch, train_loss, val_loss) = (498, 1.7095722968761737, 1.6827958822250366)\n",
      "(epoch, train_loss, val_loss) = (499, 1.7086880573859582, 1.6797288656234741)\n",
      "(epoch, train_loss, val_loss) = (500, 1.707650670638451, 1.676399827003479)\n",
      "(epoch, train_loss, val_loss) = (501, 1.7048847767022939, 1.6731683015823364)\n",
      "(epoch, train_loss, val_loss) = (502, 1.6998841854242177, 1.670393943786621)\n",
      "(epoch, train_loss, val_loss) = (503, 1.6989159950843225, 1.6683236360549927)\n",
      "(epoch, train_loss, val_loss) = (504, 1.6958183416953454, 1.6672375202178955)\n",
      "(epoch, train_loss, val_loss) = (505, 1.6966110192812407, 1.6671158075332642)\n",
      "(epoch, train_loss, val_loss) = (506, 1.6967182893019457, 1.6679021120071411)\n",
      "(epoch, train_loss, val_loss) = (507, 1.6952234781705415, 1.6693365573883057)\n",
      "(epoch, train_loss, val_loss) = (508, 1.6986212271910448, 1.6712160110473633)\n",
      "(epoch, train_loss, val_loss) = (509, 1.6998291382422814, 1.673285722732544)\n",
      "(epoch, train_loss, val_loss) = (510, 1.70660727757674, 1.6752384901046753)\n",
      "(epoch, train_loss, val_loss) = (511, 1.7095857125062208, 1.6768126487731934)\n",
      "(epoch, train_loss, val_loss) = (512, 1.7071240039972158, 1.677828073501587)\n",
      "(epoch, train_loss, val_loss) = (513, 1.7092560162911048, 1.6781437397003174)\n",
      "(epoch, train_loss, val_loss) = (514, 1.7049590349197388, 1.6776639223098755)\n",
      "(epoch, train_loss, val_loss) = (515, 1.7053285928872914, 1.6764169931411743)\n",
      "(epoch, train_loss, val_loss) = (516, 1.7046557114674494, 1.6745951175689697)\n",
      "(epoch, train_loss, val_loss) = (517, 1.7029430591143095, 1.6723402738571167)\n",
      "(epoch, train_loss, val_loss) = (518, 1.6960648756760817, 1.669895887374878)\n",
      "(epoch, train_loss, val_loss) = (519, 1.6958143894488995, 1.6675021648406982)\n",
      "(epoch, train_loss, val_loss) = (520, 1.6928184124139638, 1.665451169013977)\n",
      "(epoch, train_loss, val_loss) = (521, 1.6895743425075824, 1.6639386415481567)\n",
      "(epoch, train_loss, val_loss) = (522, 1.689724821310777, 1.6631224155426025)\n",
      "(epoch, train_loss, val_loss) = (523, 1.6926762507512019, 1.6631107330322266)\n",
      "(epoch, train_loss, val_loss) = (524, 1.6891848123990572, 1.6638813018798828)\n",
      "(epoch, train_loss, val_loss) = (525, 1.68365543622237, 1.665462851524353)\n",
      "(epoch, train_loss, val_loss) = (526, 1.6938164600959191, 1.6677086353302002)\n",
      "(epoch, train_loss, val_loss) = (527, 1.688793237392719, 1.6703722476959229)\n",
      "(epoch, train_loss, val_loss) = (528, 1.6936113650982196, 1.6731905937194824)\n",
      "(epoch, train_loss, val_loss) = (529, 1.6957224424068744, 1.6758400201797485)\n",
      "(epoch, train_loss, val_loss) = (530, 1.696185487967271, 1.6780892610549927)\n",
      "(epoch, train_loss, val_loss) = (531, 1.6981194477814894, 1.6797369718551636)\n",
      "(epoch, train_loss, val_loss) = (532, 1.7035881647696862, 1.6805967092514038)\n",
      "(epoch, train_loss, val_loss) = (533, 1.6959190276952891, 1.6805816888809204)\n",
      "(epoch, train_loss, val_loss) = (534, 1.6953818247868464, 1.6796791553497314)\n",
      "(epoch, train_loss, val_loss) = (535, 1.6998358047925508, 1.6779987812042236)\n",
      "(epoch, train_loss, val_loss) = (536, 1.6924463693912213, 1.6757363080978394)\n",
      "(epoch, train_loss, val_loss) = (537, 1.6878023606080275, 1.6731456518173218)\n",
      "(epoch, train_loss, val_loss) = (538, 1.6924052788661077, 1.6704806089401245)\n",
      "(epoch, train_loss, val_loss) = (539, 1.6873167753219604, 1.6680349111557007)\n",
      "(epoch, train_loss, val_loss) = (540, 1.6868577003479004, 1.66597318649292)\n",
      "(epoch, train_loss, val_loss) = (541, 1.6843534524624164, 1.6645296812057495)\n",
      "(epoch, train_loss, val_loss) = (542, 1.6788950975124652, 1.6638110876083374)\n",
      "(epoch, train_loss, val_loss) = (543, 1.681859438235943, 1.6639031171798706)\n",
      "(epoch, train_loss, val_loss) = (544, 1.6789863201288076, 1.664689064025879)\n",
      "(epoch, train_loss, val_loss) = (545, 1.6833685544820933, 1.6661217212677002)\n",
      "(epoch, train_loss, val_loss) = (546, 1.6847193424518292, 1.6680660247802734)\n",
      "(epoch, train_loss, val_loss) = (547, 1.6892856176082904, 1.6703152656555176)\n",
      "(epoch, train_loss, val_loss) = (548, 1.6866447100272546, 1.6726089715957642)\n",
      "(epoch, train_loss, val_loss) = (549, 1.6930182163531964, 1.6746666431427002)\n",
      "(epoch, train_loss, val_loss) = (550, 1.6995119223227868, 1.6762099266052246)\n",
      "(epoch, train_loss, val_loss) = (551, 1.6984502994097197, 1.6770561933517456)\n",
      "(epoch, train_loss, val_loss) = (552, 1.697812007023738, 1.6770144701004028)\n",
      "(epoch, train_loss, val_loss) = (553, 1.694192180266747, 1.6760300397872925)\n",
      "(epoch, train_loss, val_loss) = (554, 1.6932272269175603, 1.6740214824676514)\n",
      "(epoch, train_loss, val_loss) = (555, 1.7001958993765025, 1.6710599660873413)\n",
      "(epoch, train_loss, val_loss) = (556, 1.694586387047401, 1.6674058437347412)\n",
      "(epoch, train_loss, val_loss) = (557, 1.6899955639472375, 1.6633734703063965)\n",
      "(epoch, train_loss, val_loss) = (558, 1.6849221816429725, 1.659306287765503)\n",
      "(epoch, train_loss, val_loss) = (559, 1.682969936957726, 1.655535101890564)\n",
      "(epoch, train_loss, val_loss) = (560, 1.676773997453543, 1.6524513959884644)\n",
      "(epoch, train_loss, val_loss) = (561, 1.670457463998061, 1.6503337621688843)\n",
      "(epoch, train_loss, val_loss) = (562, 1.6709157045070941, 1.6493666172027588)\n",
      "(epoch, train_loss, val_loss) = (563, 1.6709956664305468, 1.6496297121047974)\n",
      "(epoch, train_loss, val_loss) = (564, 1.6724172647182758, 1.650949239730835)\n",
      "(epoch, train_loss, val_loss) = (565, 1.6680730122786303, 1.6530158519744873)\n",
      "(epoch, train_loss, val_loss) = (566, 1.6699604712999785, 1.6556344032287598)\n",
      "(epoch, train_loss, val_loss) = (567, 1.673262018423814, 1.658558964729309)\n",
      "(epoch, train_loss, val_loss) = (568, 1.6763073389346783, 1.661550760269165)\n",
      "(epoch, train_loss, val_loss) = (569, 1.6784782868165236, 1.6643500328063965)\n",
      "(epoch, train_loss, val_loss) = (570, 1.6818822347200835, 1.6666818857192993)\n",
      "(epoch, train_loss, val_loss) = (571, 1.6890066403609056, 1.6684246063232422)\n",
      "(epoch, train_loss, val_loss) = (572, 1.6875519844201894, 1.6695146560668945)\n",
      "(epoch, train_loss, val_loss) = (573, 1.6881325611701379, 1.6699005365371704)\n",
      "(epoch, train_loss, val_loss) = (574, 1.6862897322728083, 1.669551968574524)\n",
      "(epoch, train_loss, val_loss) = (575, 1.689986018034128, 1.6685415506362915)\n",
      "(epoch, train_loss, val_loss) = (576, 1.6885162775333111, 1.6669747829437256)\n",
      "(epoch, train_loss, val_loss) = (577, 1.6879521700052114, 1.664960503578186)\n",
      "(epoch, train_loss, val_loss) = (578, 1.6816981572371263, 1.6626355648040771)\n",
      "(epoch, train_loss, val_loss) = (579, 1.6780007435725286, 1.6601365804672241)\n",
      "(epoch, train_loss, val_loss) = (580, 1.684622076841501, 1.6575775146484375)\n",
      "(epoch, train_loss, val_loss) = (581, 1.678611892920274, 1.655137300491333)\n",
      "(epoch, train_loss, val_loss) = (582, 1.6757924189934363, 1.6531131267547607)\n",
      "(epoch, train_loss, val_loss) = (583, 1.6700750039174006, 1.6515835523605347)\n",
      "(epoch, train_loss, val_loss) = (584, 1.6728734694994414, 1.6507568359375)\n",
      "(epoch, train_loss, val_loss) = (585, 1.6688797015410204, 1.6507363319396973)\n",
      "(epoch, train_loss, val_loss) = (586, 1.6705987820258508, 1.6514732837677002)\n",
      "(epoch, train_loss, val_loss) = (587, 1.6782012994472797, 1.6528644561767578)\n",
      "(epoch, train_loss, val_loss) = (588, 1.6745700194285467, 1.6547462940216064)\n",
      "(epoch, train_loss, val_loss) = (589, 1.6786037958585298, 1.6568719148635864)\n",
      "(epoch, train_loss, val_loss) = (590, 1.6789780029883752, 1.658961534500122)\n",
      "(epoch, train_loss, val_loss) = (591, 1.6851959962111254, 1.660676121711731)\n",
      "(epoch, train_loss, val_loss) = (592, 1.6836655139923096, 1.6617753505706787)\n",
      "(epoch, train_loss, val_loss) = (593, 1.6816772772715642, 1.6620736122131348)\n",
      "(epoch, train_loss, val_loss) = (594, 1.6819445811785185, 1.6614906787872314)\n",
      "(epoch, train_loss, val_loss) = (595, 1.6859300686762884, 1.6600050926208496)\n",
      "(epoch, train_loss, val_loss) = (596, 1.6775937538880568, 1.6577394008636475)\n",
      "(epoch, train_loss, val_loss) = (597, 1.6782228213090162, 1.6548759937286377)\n",
      "(epoch, train_loss, val_loss) = (598, 1.6765867104897132, 1.6516629457473755)\n",
      "(epoch, train_loss, val_loss) = (599, 1.6704182624816895, 1.6483488082885742)\n",
      "(epoch, train_loss, val_loss) = (600, 1.665721581532405, 1.6452478170394897)\n",
      "(epoch, train_loss, val_loss) = (601, 1.6630903757535493, 1.6425853967666626)\n",
      "(epoch, train_loss, val_loss) = (602, 1.6598554299427912, 1.6405458450317383)\n",
      "(epoch, train_loss, val_loss) = (603, 1.660304803114671, 1.6392265558242798)\n",
      "(epoch, train_loss, val_loss) = (604, 1.6580733152536244, 1.6387183666229248)\n",
      "(epoch, train_loss, val_loss) = (605, 1.6581347630574153, 1.6389049291610718)\n",
      "(epoch, train_loss, val_loss) = (606, 1.6566935961063092, 1.639681100845337)\n",
      "(epoch, train_loss, val_loss) = (607, 1.6554956619556134, 1.641015648841858)\n",
      "(epoch, train_loss, val_loss) = (608, 1.6561123316104596, 1.6428133249282837)\n",
      "(epoch, train_loss, val_loss) = (609, 1.6601225046011119, 1.6449859142303467)\n",
      "(epoch, train_loss, val_loss) = (610, 1.6593563923468957, 1.6473888158798218)\n",
      "(epoch, train_loss, val_loss) = (611, 1.6673711079817553, 1.6499238014221191)\n",
      "(epoch, train_loss, val_loss) = (612, 1.6673424519025362, 1.6524615287780762)\n",
      "(epoch, train_loss, val_loss) = (613, 1.6728535432081957, 1.6548054218292236)\n",
      "(epoch, train_loss, val_loss) = (614, 1.672110392497136, 1.6568154096603394)\n",
      "(epoch, train_loss, val_loss) = (615, 1.6716924630678618, 1.6583662033081055)\n",
      "(epoch, train_loss, val_loss) = (616, 1.6742668426953828, 1.6593332290649414)\n",
      "(epoch, train_loss, val_loss) = (617, 1.67514522259052, 1.659575343132019)\n",
      "(epoch, train_loss, val_loss) = (618, 1.6756476714060857, 1.6590808629989624)\n",
      "(epoch, train_loss, val_loss) = (619, 1.6780111422905555, 1.6578078269958496)\n",
      "(epoch, train_loss, val_loss) = (620, 1.6685312252778273, 1.6557925939559937)\n",
      "(epoch, train_loss, val_loss) = (621, 1.6704841485390296, 1.6531931161880493)\n",
      "(epoch, train_loss, val_loss) = (622, 1.6702076196670532, 1.6502349376678467)\n",
      "(epoch, train_loss, val_loss) = (623, 1.6660070786109338, 1.6471971273422241)\n",
      "(epoch, train_loss, val_loss) = (624, 1.6641604166764479, 1.6443727016448975)\n",
      "(epoch, train_loss, val_loss) = (625, 1.6605350237626295, 1.6420451402664185)\n",
      "(epoch, train_loss, val_loss) = (626, 1.6612693346463716, 1.6403710842132568)\n",
      "(epoch, train_loss, val_loss) = (627, 1.6607164144515991, 1.6394342184066772)\n",
      "(epoch, train_loss, val_loss) = (628, 1.6589423418045044, 1.6393033266067505)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch, train_loss, val_loss) = (629, 1.6609257459640503, 1.6398968696594238)\n",
      "(epoch, train_loss, val_loss) = (630, 1.6604338517555823, 1.6410287618637085)\n",
      "(epoch, train_loss, val_loss) = (631, 1.6611034961847158, 1.6425939798355103)\n",
      "(epoch, train_loss, val_loss) = (632, 1.6601973405251136, 1.6443535089492798)\n",
      "(epoch, train_loss, val_loss) = (633, 1.6651317523075984, 1.6460869312286377)\n",
      "(epoch, train_loss, val_loss) = (634, 1.6653471359839807, 1.6475613117218018)\n",
      "(epoch, train_loss, val_loss) = (635, 1.6664625039467444, 1.6485812664031982)\n",
      "(epoch, train_loss, val_loss) = (636, 1.6624984374413123, 1.6490554809570312)\n",
      "(epoch, train_loss, val_loss) = (637, 1.6663801119877741, 1.6489887237548828)\n",
      "(epoch, train_loss, val_loss) = (638, 1.662171877347506, 1.6483699083328247)\n",
      "(epoch, train_loss, val_loss) = (639, 1.6626113378084624, 1.6471916437149048)\n",
      "(epoch, train_loss, val_loss) = (640, 1.657773182942317, 1.6455806493759155)\n",
      "(epoch, train_loss, val_loss) = (641, 1.6601163057180552, 1.6436920166015625)\n",
      "(epoch, train_loss, val_loss) = (642, 1.6590764705951397, 1.6416925191879272)\n",
      "(epoch, train_loss, val_loss) = (643, 1.659240988584665, 1.6397594213485718)\n",
      "(epoch, train_loss, val_loss) = (644, 1.6514589694830089, 1.6380903720855713)\n",
      "(epoch, train_loss, val_loss) = (645, 1.6550199435307429, 1.636820912361145)\n",
      "(epoch, train_loss, val_loss) = (646, 1.6508236114795392, 1.636073112487793)\n",
      "(epoch, train_loss, val_loss) = (647, 1.6506338211206288, 1.6359552145004272)\n",
      "(epoch, train_loss, val_loss) = (648, 1.6525422793168287, 1.6365406513214111)\n",
      "(epoch, train_loss, val_loss) = (649, 1.6518536255909846, 1.637708067893982)\n",
      "(epoch, train_loss, val_loss) = (650, 1.6518112054237952, 1.6393835544586182)\n",
      "(epoch, train_loss, val_loss) = (651, 1.655857223730821, 1.6414228677749634)\n",
      "(epoch, train_loss, val_loss) = (652, 1.6536050393031194, 1.643725037574768)\n",
      "(epoch, train_loss, val_loss) = (653, 1.6578751710745006, 1.6460593938827515)\n",
      "(epoch, train_loss, val_loss) = (654, 1.6634629506331224, 1.648207426071167)\n",
      "(epoch, train_loss, val_loss) = (655, 1.6597114709707408, 1.6499806642532349)\n",
      "(epoch, train_loss, val_loss) = (656, 1.6603126525878906, 1.6512562036514282)\n",
      "(epoch, train_loss, val_loss) = (657, 1.6623425850501428, 1.6518816947937012)\n",
      "(epoch, train_loss, val_loss) = (658, 1.6666542199941783, 1.6517714262008667)\n",
      "(epoch, train_loss, val_loss) = (659, 1.6636657072947576, 1.6509217023849487)\n",
      "(epoch, train_loss, val_loss) = (660, 1.6618893880110521, 1.6493877172470093)\n",
      "(epoch, train_loss, val_loss) = (661, 1.6618891129126916, 1.6472889184951782)\n",
      "(epoch, train_loss, val_loss) = (662, 1.6601415230677679, 1.6448216438293457)\n",
      "(epoch, train_loss, val_loss) = (663, 1.652178406715393, 1.642079472541809)\n",
      "(epoch, train_loss, val_loss) = (664, 1.652270693045396, 1.639290452003479)\n",
      "(epoch, train_loss, val_loss) = (665, 1.6497293160511897, 1.6366262435913086)\n",
      "(epoch, train_loss, val_loss) = (666, 1.649621816781851, 1.63431978225708)\n",
      "(epoch, train_loss, val_loss) = (667, 1.6479243040084839, 1.6324090957641602)\n",
      "(epoch, train_loss, val_loss) = (668, 1.6430836732570941, 1.6310005187988281)\n",
      "(epoch, train_loss, val_loss) = (669, 1.644910216331482, 1.6301777362823486)\n",
      "(epoch, train_loss, val_loss) = (670, 1.6430609409625714, 1.6299363374710083)\n",
      "(epoch, train_loss, val_loss) = (671, 1.6448170496867254, 1.630199909210205)\n",
      "(epoch, train_loss, val_loss) = (672, 1.644417377618643, 1.6309386491775513)\n",
      "(epoch, train_loss, val_loss) = (673, 1.6408500304588904, 1.6320438385009766)\n",
      "(epoch, train_loss, val_loss) = (674, 1.6478709074167104, 1.6334962844848633)\n",
      "(epoch, train_loss, val_loss) = (675, 1.649819355744582, 1.6351488828659058)\n",
      "(epoch, train_loss, val_loss) = (676, 1.6456862688064575, 1.636918544769287)\n",
      "(epoch, train_loss, val_loss) = (677, 1.6529197417772734, 1.6386791467666626)\n",
      "(epoch, train_loss, val_loss) = (678, 1.6521333089241614, 1.6402957439422607)\n",
      "(epoch, train_loss, val_loss) = (679, 1.6508887456013606, 1.6416716575622559)\n",
      "(epoch, train_loss, val_loss) = (680, 1.6532349861585176, 1.6426959037780762)\n",
      "(epoch, train_loss, val_loss) = (681, 1.6578010412362905, 1.6432044506072998)\n",
      "(epoch, train_loss, val_loss) = (682, 1.6583612056878896, 1.6430789232254028)\n",
      "(epoch, train_loss, val_loss) = (683, 1.6586222373522246, 1.6422770023345947)\n",
      "(epoch, train_loss, val_loss) = (684, 1.6553134734813983, 1.640778660774231)\n",
      "(epoch, train_loss, val_loss) = (685, 1.6565513519140391, 1.6386338472366333)\n",
      "(epoch, train_loss, val_loss) = (686, 1.653308547460116, 1.635973334312439)\n",
      "(epoch, train_loss, val_loss) = (687, 1.6507942859943097, 1.6329996585845947)\n",
      "(epoch, train_loss, val_loss) = (688, 1.651410359602708, 1.629894733428955)\n",
      "(epoch, train_loss, val_loss) = (689, 1.6448199657293467, 1.6269451379776)\n",
      "(epoch, train_loss, val_loss) = (690, 1.6410673031440148, 1.6243515014648438)\n",
      "(epoch, train_loss, val_loss) = (691, 1.6382109476969793, 1.6223351955413818)\n",
      "(epoch, train_loss, val_loss) = (692, 1.6347448550737822, 1.6210960149765015)\n",
      "(epoch, train_loss, val_loss) = (693, 1.6331350344877977, 1.6207417249679565)\n",
      "(epoch, train_loss, val_loss) = (694, 1.6325983084165132, 1.6213089227676392)\n",
      "(epoch, train_loss, val_loss) = (695, 1.6323161400281465, 1.6226528882980347)\n",
      "(epoch, train_loss, val_loss) = (696, 1.6353654127854567, 1.6246590614318848)\n",
      "(epoch, train_loss, val_loss) = (697, 1.6383933562498827, 1.6271822452545166)\n",
      "(epoch, train_loss, val_loss) = (698, 1.6422687768936157, 1.6300411224365234)\n",
      "(epoch, train_loss, val_loss) = (699, 1.6408014939381526, 1.6329991817474365)\n",
      "(epoch, train_loss, val_loss) = (700, 1.6403656189258282, 1.6358439922332764)\n",
      "(epoch, train_loss, val_loss) = (701, 1.6446167689103346, 1.6384402513504028)\n",
      "(epoch, train_loss, val_loss) = (702, 1.6466882503949678, 1.6405644416809082)\n",
      "(epoch, train_loss, val_loss) = (703, 1.6498462420243483, 1.6420495510101318)\n",
      "(epoch, train_loss, val_loss) = (704, 1.6515538233977098, 1.6428014039993286)\n",
      "(epoch, train_loss, val_loss) = (705, 1.651602864265442, 1.6427550315856934)\n",
      "(epoch, train_loss, val_loss) = (706, 1.6513216678912823, 1.6419405937194824)\n",
      "(epoch, train_loss, val_loss) = (707, 1.644259782937857, 1.640406847000122)\n",
      "(epoch, train_loss, val_loss) = (708, 1.6451657185187707, 1.638258457183838)\n",
      "(epoch, train_loss, val_loss) = (709, 1.6415286706044123, 1.6356500387191772)\n",
      "(epoch, train_loss, val_loss) = (710, 1.64511854831989, 1.6327574253082275)\n",
      "(epoch, train_loss, val_loss) = (711, 1.6382092604270349, 1.6297858953475952)\n",
      "(epoch, train_loss, val_loss) = (712, 1.6408563944009633, 1.627017617225647)\n",
      "(epoch, train_loss, val_loss) = (713, 1.6401257239855254, 1.6246308088302612)\n",
      "(epoch, train_loss, val_loss) = (714, 1.634479806973384, 1.6228182315826416)\n",
      "(epoch, train_loss, val_loss) = (715, 1.6343725277827337, 1.6217073202133179)\n",
      "(epoch, train_loss, val_loss) = (716, 1.6353758665231557, 1.6213051080703735)\n",
      "(epoch, train_loss, val_loss) = (717, 1.6309754665081317, 1.6216604709625244)\n",
      "(epoch, train_loss, val_loss) = (718, 1.6327547751940215, 1.6227030754089355)\n",
      "(epoch, train_loss, val_loss) = (719, 1.6319569716086755, 1.6242951154708862)\n",
      "(epoch, train_loss, val_loss) = (720, 1.6366610343639667, 1.6262462139129639)\n",
      "(epoch, train_loss, val_loss) = (721, 1.6394196015137892, 1.6283310651779175)\n",
      "(epoch, train_loss, val_loss) = (722, 1.642179984312791, 1.6303489208221436)\n",
      "(epoch, train_loss, val_loss) = (723, 1.6442082203351533, 1.6321353912353516)\n",
      "(epoch, train_loss, val_loss) = (724, 1.6469779106286855, 1.6335750818252563)\n",
      "(epoch, train_loss, val_loss) = (725, 1.644912628027109, 1.6345752477645874)\n",
      "(epoch, train_loss, val_loss) = (726, 1.6435808493540838, 1.635145902633667)\n",
      "(epoch, train_loss, val_loss) = (727, 1.6459330595456636, 1.635337471961975)\n",
      "(epoch, train_loss, val_loss) = (728, 1.6487265825271606, 1.6352014541625977)\n",
      "(epoch, train_loss, val_loss) = (729, 1.6445103058448205, 1.6348392963409424)\n",
      "(epoch, train_loss, val_loss) = (730, 1.6400191692205577, 1.634337067604065)\n",
      "(epoch, train_loss, val_loss) = (731, 1.642662763595581, 1.6337158679962158)\n",
      "(epoch, train_loss, val_loss) = (732, 1.6407572031021118, 1.6330150365829468)\n",
      "(epoch, train_loss, val_loss) = (733, 1.644038622195904, 1.6321479082107544)\n",
      "(epoch, train_loss, val_loss) = (734, 1.636876078752371, 1.6310375928878784)\n",
      "(epoch, train_loss, val_loss) = (735, 1.6357131096032949, 1.6296311616897583)\n",
      "(epoch, train_loss, val_loss) = (736, 1.6327420656497662, 1.6279460191726685)\n",
      "(epoch, train_loss, val_loss) = (737, 1.6326411137214074, 1.6259760856628418)\n",
      "(epoch, train_loss, val_loss) = (738, 1.632313297345088, 1.6237807273864746)\n",
      "(epoch, train_loss, val_loss) = (739, 1.6321878341528087, 1.6214919090270996)\n",
      "(epoch, train_loss, val_loss) = (740, 1.6244282814172597, 1.6192915439605713)\n",
      "(epoch, train_loss, val_loss) = (741, 1.6198605665793786, 1.6174263954162598)\n",
      "(epoch, train_loss, val_loss) = (742, 1.6245714792838464, 1.616124153137207)\n",
      "(epoch, train_loss, val_loss) = (743, 1.6194537969735951, 1.6155372858047485)\n",
      "(epoch, train_loss, val_loss) = (744, 1.6177738446455736, 1.615803837776184)\n",
      "(epoch, train_loss, val_loss) = (745, 1.6211782968961275, 1.6169395446777344)\n",
      "(epoch, train_loss, val_loss) = (746, 1.6228828980372503, 1.618889331817627)\n",
      "(epoch, train_loss, val_loss) = (747, 1.629231883929326, 1.6215336322784424)\n",
      "(epoch, train_loss, val_loss) = (748, 1.6299227567819448, 1.6246094703674316)\n",
      "(epoch, train_loss, val_loss) = (749, 1.632844301370474, 1.627772331237793)\n",
      "(epoch, train_loss, val_loss) = (750, 1.6368336310753455, 1.6307138204574585)\n",
      "(epoch, train_loss, val_loss) = (751, 1.6403296544001653, 1.6331666707992554)\n",
      "(epoch, train_loss, val_loss) = (752, 1.6382294159669142, 1.634909987449646)\n",
      "(epoch, train_loss, val_loss) = (753, 1.643585205078125, 1.6358126401901245)\n",
      "(epoch, train_loss, val_loss) = (754, 1.6429917812347412, 1.6358290910720825)\n",
      "(epoch, train_loss, val_loss) = (755, 1.6405121454825768, 1.6350206136703491)\n",
      "(epoch, train_loss, val_loss) = (756, 1.638433658159696, 1.6335843801498413)\n",
      "(epoch, train_loss, val_loss) = (757, 1.6379959399883564, 1.6317861080169678)\n",
      "(epoch, train_loss, val_loss) = (758, 1.6320302486419678, 1.6298649311065674)\n",
      "(epoch, train_loss, val_loss) = (759, 1.6331257636730487, 1.628071904182434)\n",
      "(epoch, train_loss, val_loss) = (760, 1.6264778559024518, 1.6265965700149536)\n",
      "(epoch, train_loss, val_loss) = (761, 1.6255579178149884, 1.6255431175231934)\n",
      "(epoch, train_loss, val_loss) = (762, 1.6230077468431914, 1.6250265836715698)\n",
      "(epoch, train_loss, val_loss) = (763, 1.6203520848200872, 1.6250386238098145)\n",
      "(epoch, train_loss, val_loss) = (764, 1.6267306621258075, 1.6254202127456665)\n",
      "(epoch, train_loss, val_loss) = (765, 1.6271622272638173, 1.625993013381958)\n",
      "(epoch, train_loss, val_loss) = (766, 1.6278951718257024, 1.6266030073165894)\n",
      "(epoch, train_loss, val_loss) = (767, 1.6286395329695482, 1.627089023590088)\n",
      "(epoch, train_loss, val_loss) = (768, 1.6270179106638982, 1.6273325681686401)\n",
      "(epoch, train_loss, val_loss) = (769, 1.625911254149217, 1.6272644996643066)\n",
      "(epoch, train_loss, val_loss) = (770, 1.6293994738505437, 1.6268677711486816)\n",
      "(epoch, train_loss, val_loss) = (771, 1.626616881443904, 1.6261889934539795)\n",
      "(epoch, train_loss, val_loss) = (772, 1.6271991271239061, 1.6253689527511597)\n",
      "(epoch, train_loss, val_loss) = (773, 1.6288333581044123, 1.6245019435882568)\n",
      "(epoch, train_loss, val_loss) = (774, 1.6304468283286462, 1.6236393451690674)\n",
      "(epoch, train_loss, val_loss) = (775, 1.6274984158002412, 1.6228610277175903)\n",
      "(epoch, train_loss, val_loss) = (776, 1.6226416184351995, 1.6221047639846802)\n",
      "(epoch, train_loss, val_loss) = (777, 1.626077184310326, 1.6213276386260986)\n",
      "(epoch, train_loss, val_loss) = (778, 1.6270239903376653, 1.6204469203948975)\n",
      "(epoch, train_loss, val_loss) = (779, 1.625456223121056, 1.6193493604660034)\n",
      "(epoch, train_loss, val_loss) = (780, 1.6274837713975172, 1.6179591417312622)\n",
      "(epoch, train_loss, val_loss) = (781, 1.627903333077064, 1.6162912845611572)\n",
      "(epoch, train_loss, val_loss) = (782, 1.6233363976845374, 1.6144195795059204)\n",
      "(epoch, train_loss, val_loss) = (783, 1.6244617517177875, 1.612553358078003)\n",
      "(epoch, train_loss, val_loss) = (784, 1.6203412917944102, 1.6108778715133667)\n",
      "(epoch, train_loss, val_loss) = (785, 1.6180832019219031, 1.6096749305725098)\n",
      "(epoch, train_loss, val_loss) = (786, 1.617446468426631, 1.6092106103897095)\n",
      "(epoch, train_loss, val_loss) = (787, 1.6151688007208018, 1.609662413597107)\n",
      "(epoch, train_loss, val_loss) = (788, 1.615142400448139, 1.6110860109329224)\n",
      "(epoch, train_loss, val_loss) = (789, 1.6186543519680316, 1.6134226322174072)\n",
      "(epoch, train_loss, val_loss) = (790, 1.6202525267234216, 1.6165450811386108)\n",
      "(epoch, train_loss, val_loss) = (791, 1.6232862564233632, 1.6202894449234009)\n",
      "(epoch, train_loss, val_loss) = (792, 1.6208255657782922, 1.624293565750122)\n",
      "(epoch, train_loss, val_loss) = (793, 1.6265883445739746, 1.6281392574310303)\n",
      "(epoch, train_loss, val_loss) = (794, 1.633472910294166, 1.631422996520996)\n",
      "(epoch, train_loss, val_loss) = (795, 1.6335814274274385, 1.6338295936584473)\n",
      "(epoch, train_loss, val_loss) = (796, 1.6339234480491052, 1.6350435018539429)\n",
      "(epoch, train_loss, val_loss) = (797, 1.6336622880055354, 1.6348661184310913)\n",
      "(epoch, train_loss, val_loss) = (798, 1.6345974115224986, 1.633269190788269)\n",
      "(epoch, train_loss, val_loss) = (799, 1.6305093398460975, 1.6303781270980835)\n",
      "(epoch, train_loss, val_loss) = (800, 1.6277300669596746, 1.6265169382095337)\n",
      "(epoch, train_loss, val_loss) = (801, 1.630253030703618, 1.6221004724502563)\n",
      "(epoch, train_loss, val_loss) = (802, 1.6209233540755053, 1.6175791025161743)\n",
      "(epoch, train_loss, val_loss) = (803, 1.618537004177387, 1.6134213209152222)\n",
      "(epoch, train_loss, val_loss) = (804, 1.6095624978725727, 1.6099635362625122)\n",
      "(epoch, train_loss, val_loss) = (805, 1.614204691006587, 1.6075688600540161)\n",
      "(epoch, train_loss, val_loss) = (806, 1.609578040929941, 1.6064836978912354)\n",
      "(epoch, train_loss, val_loss) = (807, 1.61331372994643, 1.6066982746124268)\n",
      "(epoch, train_loss, val_loss) = (808, 1.6100010321690486, 1.6080424785614014)\n",
      "(epoch, train_loss, val_loss) = (809, 1.611323750936068, 1.6102228164672852)\n",
      "(epoch, train_loss, val_loss) = (810, 1.619465030156649, 1.6128937005996704)\n",
      "(epoch, train_loss, val_loss) = (811, 1.6184743826205914, 1.6156578063964844)\n",
      "(epoch, train_loss, val_loss) = (812, 1.6235423271472638, 1.6181671619415283)\n",
      "(epoch, train_loss, val_loss) = (813, 1.6262386762178862, 1.6201120615005493)\n",
      "(epoch, train_loss, val_loss) = (814, 1.6267174207247221, 1.6212531328201294)\n",
      "(epoch, train_loss, val_loss) = (815, 1.6261844084813044, 1.6215312480926514)\n",
      "(epoch, train_loss, val_loss) = (816, 1.6247660563542292, 1.621029257774353)\n",
      "(epoch, train_loss, val_loss) = (817, 1.6280983686447144, 1.6199889183044434)\n",
      "(epoch, train_loss, val_loss) = (818, 1.6238859249995306, 1.6186931133270264)\n",
      "(epoch, train_loss, val_loss) = (819, 1.6223334624217107, 1.617446780204773)\n",
      "(epoch, train_loss, val_loss) = (820, 1.6167208323111901, 1.616611123085022)\n",
      "(epoch, train_loss, val_loss) = (821, 1.614790338736314, 1.6163699626922607)\n",
      "(epoch, train_loss, val_loss) = (822, 1.6142450662759633, 1.616765022277832)\n",
      "(epoch, train_loss, val_loss) = (823, 1.6153177481431227, 1.6176373958587646)\n",
      "(epoch, train_loss, val_loss) = (824, 1.6174523738714366, 1.6188271045684814)\n",
      "(epoch, train_loss, val_loss) = (825, 1.6204980886899507, 1.620047926902771)\n",
      "(epoch, train_loss, val_loss) = (826, 1.6199018680132353, 1.6208958625793457)\n",
      "(epoch, train_loss, val_loss) = (827, 1.6229713238202608, 1.6210583448410034)\n",
      "(epoch, train_loss, val_loss) = (828, 1.622270107269287, 1.620389461517334)\n",
      "(epoch, train_loss, val_loss) = (829, 1.6182349278376653, 1.6188106536865234)\n",
      "(epoch, train_loss, val_loss) = (830, 1.6174873205331655, 1.6163783073425293)\n",
      "(epoch, train_loss, val_loss) = (831, 1.6124458771485548, 1.6134148836135864)\n",
      "(epoch, train_loss, val_loss) = (832, 1.614366338803218, 1.610357403755188)\n",
      "(epoch, train_loss, val_loss) = (833, 1.6079463225144606, 1.6076912879943848)\n",
      "(epoch, train_loss, val_loss) = (834, 1.6075805792441735, 1.6058870553970337)\n",
      "(epoch, train_loss, val_loss) = (835, 1.6099183467718272, 1.6051993370056152)\n",
      "(epoch, train_loss, val_loss) = (836, 1.6129475373488207, 1.605890154838562)\n",
      "(epoch, train_loss, val_loss) = (837, 1.6100755746547992, 1.6079304218292236)\n",
      "(epoch, train_loss, val_loss) = (838, 1.619466864145719, 1.6111304759979248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch, train_loss, val_loss) = (839, 1.6193878283867469, 1.615108847618103)\n",
      "(epoch, train_loss, val_loss) = (840, 1.6220834988814135, 1.6193931102752686)\n",
      "(epoch, train_loss, val_loss) = (841, 1.6319599151611328, 1.6233693361282349)\n",
      "(epoch, train_loss, val_loss) = (842, 1.6299093594917884, 1.6265281438827515)\n",
      "(epoch, train_loss, val_loss) = (843, 1.6360479776675885, 1.6284362077713013)\n",
      "(epoch, train_loss, val_loss) = (844, 1.6329841522070079, 1.6287336349487305)\n",
      "(epoch, train_loss, val_loss) = (845, 1.6408184950168316, 1.6272788047790527)\n",
      "(epoch, train_loss, val_loss) = (846, 1.6363497422291682, 1.6241225004196167)\n",
      "(epoch, train_loss, val_loss) = (847, 1.635703912148109, 1.619581699371338)\n",
      "(epoch, train_loss, val_loss) = (848, 1.6262711194845347, 1.6140637397766113)\n",
      "(epoch, train_loss, val_loss) = (849, 1.6208719565318181, 1.6081877946853638)\n",
      "(epoch, train_loss, val_loss) = (850, 1.6179098441050603, 1.6025288105010986)\n",
      "(epoch, train_loss, val_loss) = (851, 1.6042848183558538, 1.5977152585983276)\n",
      "(epoch, train_loss, val_loss) = (852, 1.6014381005213811, 1.5943357944488525)\n",
      "(epoch, train_loss, val_loss) = (853, 1.5993991173230684, 1.592848777770996)\n",
      "(epoch, train_loss, val_loss) = (854, 1.5982353320488563, 1.5932711362838745)\n",
      "(epoch, train_loss, val_loss) = (855, 1.599881227199848, 1.5955721139907837)\n",
      "(epoch, train_loss, val_loss) = (856, 1.5976361403098474, 1.5994343757629395)\n",
      "(epoch, train_loss, val_loss) = (857, 1.6047265071135302, 1.6044769287109375)\n",
      "(epoch, train_loss, val_loss) = (858, 1.60841861137977, 1.6101806163787842)\n",
      "(epoch, train_loss, val_loss) = (859, 1.6089740349696233, 1.615917682647705)\n",
      "(epoch, train_loss, val_loss) = (860, 1.6120930176514845, 1.6210893392562866)\n",
      "(epoch, train_loss, val_loss) = (861, 1.6169560047296376, 1.6250725984573364)\n",
      "(epoch, train_loss, val_loss) = (862, 1.6217839259367723, 1.6275302171707153)\n",
      "(epoch, train_loss, val_loss) = (863, 1.6201588740715613, 1.628310203552246)\n",
      "(epoch, train_loss, val_loss) = (864, 1.6243349405435414, 1.6274476051330566)\n",
      "(epoch, train_loss, val_loss) = (865, 1.623286797450139, 1.6252832412719727)\n",
      "(epoch, train_loss, val_loss) = (866, 1.6201041386677668, 1.6222513914108276)\n",
      "(epoch, train_loss, val_loss) = (867, 1.6184838368342473, 1.6188870668411255)\n",
      "(epoch, train_loss, val_loss) = (868, 1.6141186127295861, 1.6157408952713013)\n",
      "(epoch, train_loss, val_loss) = (869, 1.614832960642301, 1.6133452653884888)\n",
      "(epoch, train_loss, val_loss) = (870, 1.6108856934767504, 1.612084150314331)\n",
      "(epoch, train_loss, val_loss) = (871, 1.608525542112497, 1.6120737791061401)\n",
      "(epoch, train_loss, val_loss) = (872, 1.612216830253601, 1.6132071018218994)\n",
      "(epoch, train_loss, val_loss) = (873, 1.6113338653857892, 1.6151385307312012)\n",
      "(epoch, train_loss, val_loss) = (874, 1.620178406055157, 1.6173791885375977)\n",
      "(epoch, train_loss, val_loss) = (875, 1.6238449536837065, 1.6193277835845947)\n",
      "(epoch, train_loss, val_loss) = (876, 1.626342223240779, 1.6204639673233032)\n",
      "(epoch, train_loss, val_loss) = (877, 1.6238933893350453, 1.6203769445419312)\n",
      "(epoch, train_loss, val_loss) = (878, 1.620559573173523, 1.618941307067871)\n",
      "(epoch, train_loss, val_loss) = (879, 1.6219448493077204, 1.6162234544754028)\n",
      "(epoch, train_loss, val_loss) = (880, 1.6160747454716609, 1.612493634223938)\n",
      "(epoch, train_loss, val_loss) = (881, 1.6138053857363188, 1.6083316802978516)\n",
      "(epoch, train_loss, val_loss) = (882, 1.6039698857527513, 1.6043987274169922)\n",
      "(epoch, train_loss, val_loss) = (883, 1.6024615948016827, 1.6013154983520508)\n",
      "(epoch, train_loss, val_loss) = (884, 1.5991833301690908, 1.599604606628418)\n",
      "(epoch, train_loss, val_loss) = (885, 1.5987394131146944, 1.5995978116989136)\n",
      "(epoch, train_loss, val_loss) = (886, 1.6010335775522084, 1.6014338731765747)\n",
      "(epoch, train_loss, val_loss) = (887, 1.5999054541954627, 1.6050843000411987)\n",
      "(epoch, train_loss, val_loss) = (888, 1.6053309990809514, 1.610227346420288)\n",
      "(epoch, train_loss, val_loss) = (889, 1.613272685271043, 1.6163508892059326)\n",
      "(epoch, train_loss, val_loss) = (890, 1.613552790421706, 1.6228357553482056)\n",
      "(epoch, train_loss, val_loss) = (891, 1.61982654608213, 1.6289652585983276)\n",
      "(epoch, train_loss, val_loss) = (892, 1.6247258553138146, 1.6340421438217163)\n",
      "(epoch, train_loss, val_loss) = (893, 1.6258264688345103, 1.6376163959503174)\n",
      "(epoch, train_loss, val_loss) = (894, 1.6284980682226329, 1.6392301321029663)\n",
      "(epoch, train_loss, val_loss) = (895, 1.630296881382282, 1.6386823654174805)\n",
      "(epoch, train_loss, val_loss) = (896, 1.629826307296753, 1.6360423564910889)\n",
      "(epoch, train_loss, val_loss) = (897, 1.6270430088043213, 1.6315995454788208)\n",
      "(epoch, train_loss, val_loss) = (898, 1.619811250613286, 1.6257926225662231)\n",
      "(epoch, train_loss, val_loss) = (899, 1.6178434445307806, 1.6191391944885254)\n",
      "(epoch, train_loss, val_loss) = (900, 1.6106255788069506, 1.612430214881897)\n",
      "(epoch, train_loss, val_loss) = (901, 1.6048277158003588, 1.606300711631775)\n",
      "(epoch, train_loss, val_loss) = (902, 1.6012660356668325, 1.6012576818466187)\n",
      "(epoch, train_loss, val_loss) = (903, 1.5946752658257117, 1.5977736711502075)\n",
      "(epoch, train_loss, val_loss) = (904, 1.5925768613815308, 1.5960959196090698)\n",
      "(epoch, train_loss, val_loss) = (905, 1.5974113666094267, 1.5963753461837769)\n",
      "(epoch, train_loss, val_loss) = (906, 1.5953443233783429, 1.5984621047973633)\n",
      "(epoch, train_loss, val_loss) = (907, 1.6015757414010854, 1.6020891666412354)\n",
      "(epoch, train_loss, val_loss) = (908, 1.6048731895593495, 1.6067678928375244)\n",
      "(epoch, train_loss, val_loss) = (909, 1.607942012640146, 1.6119054555892944)\n",
      "(epoch, train_loss, val_loss) = (910, 1.6195346392118013, 1.616815447807312)\n",
      "(epoch, train_loss, val_loss) = (911, 1.6198302232302153, 1.620847225189209)\n",
      "(epoch, train_loss, val_loss) = (912, 1.628514940922077, 1.623460292816162)\n",
      "(epoch, train_loss, val_loss) = (913, 1.6296374247624323, 1.624252200126648)\n",
      "(epoch, train_loss, val_loss) = (914, 1.62975405729734, 1.623030662536621)\n",
      "(epoch, train_loss, val_loss) = (915, 1.627784719833961, 1.619888424873352)\n",
      "(epoch, train_loss, val_loss) = (916, 1.624948446567242, 1.6152263879776)\n",
      "(epoch, train_loss, val_loss) = (917, 1.622329803613516, 1.6096376180648804)\n",
      "(epoch, train_loss, val_loss) = (918, 1.6088265363986676, 1.6039098501205444)\n",
      "(epoch, train_loss, val_loss) = (919, 1.6018199278758123, 1.5986970663070679)\n",
      "(epoch, train_loss, val_loss) = (920, 1.5978300112944384, 1.5948020219802856)\n",
      "(epoch, train_loss, val_loss) = (921, 1.591317708675678, 1.5927257537841797)\n",
      "(epoch, train_loss, val_loss) = (922, 1.587001818877, 1.5927472114562988)\n",
      "(epoch, train_loss, val_loss) = (923, 1.5901565093260546, 1.5949430465698242)\n",
      "(epoch, train_loss, val_loss) = (924, 1.5915274161558886, 1.59907066822052)\n",
      "(epoch, train_loss, val_loss) = (925, 1.5957110570027278, 1.604720950126648)\n",
      "(epoch, train_loss, val_loss) = (926, 1.5988097282556386, 1.6113168001174927)\n",
      "(epoch, train_loss, val_loss) = (927, 1.6022968200536876, 1.6181401014328003)\n",
      "(epoch, train_loss, val_loss) = (928, 1.6170445772317739, 1.6245371103286743)\n",
      "(epoch, train_loss, val_loss) = (929, 1.6154476404190063, 1.6298606395721436)\n",
      "(epoch, train_loss, val_loss) = (930, 1.6231844516900868, 1.6336479187011719)\n",
      "(epoch, train_loss, val_loss) = (931, 1.6238040740673358, 1.6355692148208618)\n",
      "(epoch, train_loss, val_loss) = (932, 1.6254505285849938, 1.6354875564575195)\n",
      "(epoch, train_loss, val_loss) = (933, 1.6280490435086763, 1.6335351467132568)\n",
      "(epoch, train_loss, val_loss) = (934, 1.6252800226211548, 1.6300089359283447)\n",
      "(epoch, train_loss, val_loss) = (935, 1.6173022251862745, 1.625386118888855)\n",
      "(epoch, train_loss, val_loss) = (936, 1.6155434113282423, 1.6202222108840942)\n",
      "(epoch, train_loss, val_loss) = (937, 1.6121770968803992, 1.615099549293518)\n",
      "(epoch, train_loss, val_loss) = (938, 1.60423966554495, 1.610511064529419)\n",
      "(epoch, train_loss, val_loss) = (939, 1.6009778059445894, 1.6068387031555176)\n",
      "(epoch, train_loss, val_loss) = (940, 1.601162497813885, 1.6044164896011353)\n",
      "(epoch, train_loss, val_loss) = (941, 1.6001980304718018, 1.603293538093567)\n",
      "(epoch, train_loss, val_loss) = (942, 1.6001091645314143, 1.6033573150634766)\n",
      "(epoch, train_loss, val_loss) = (943, 1.6016532182693481, 1.6044543981552124)\n",
      "(epoch, train_loss, val_loss) = (944, 1.6022595167160034, 1.606177806854248)\n",
      "(epoch, train_loss, val_loss) = (945, 1.6062804460525513, 1.608107328414917)\n",
      "(epoch, train_loss, val_loss) = (946, 1.607492547768813, 1.6098209619522095)\n",
      "(epoch, train_loss, val_loss) = (947, 1.6096856043888972, 1.6110056638717651)\n",
      "(epoch, train_loss, val_loss) = (948, 1.6150253827755268, 1.6113537549972534)\n",
      "(epoch, train_loss, val_loss) = (949, 1.6126378224446223, 1.6106534004211426)\n",
      "(epoch, train_loss, val_loss) = (950, 1.614292548252986, 1.6088346242904663)\n",
      "(epoch, train_loss, val_loss) = (951, 1.6078886160483727, 1.6059715747833252)\n",
      "(epoch, train_loss, val_loss) = (952, 1.6073657274246216, 1.6023259162902832)\n",
      "(epoch, train_loss, val_loss) = (953, 1.6031047105789185, 1.5983219146728516)\n",
      "(epoch, train_loss, val_loss) = (954, 1.5972965038739717, 1.5944175720214844)\n",
      "(epoch, train_loss, val_loss) = (955, 1.5944784512886634, 1.5911316871643066)\n",
      "(epoch, train_loss, val_loss) = (956, 1.5944078335395226, 1.5888665914535522)\n",
      "(epoch, train_loss, val_loss) = (957, 1.5917020302552443, 1.588020920753479)\n",
      "(epoch, train_loss, val_loss) = (958, 1.5886230927247267, 1.5887949466705322)\n",
      "(epoch, train_loss, val_loss) = (959, 1.5878882774939904, 1.591299057006836)\n",
      "(epoch, train_loss, val_loss) = (960, 1.5892437329659095, 1.595373511314392)\n",
      "(epoch, train_loss, val_loss) = (961, 1.5944674106744618, 1.6007753610610962)\n",
      "(epoch, train_loss, val_loss) = (962, 1.6054149591005766, 1.6069791316986084)\n",
      "(epoch, train_loss, val_loss) = (963, 1.6064357849267812, 1.6134873628616333)\n",
      "(epoch, train_loss, val_loss) = (964, 1.6119708372996404, 1.619741439819336)\n",
      "(epoch, train_loss, val_loss) = (965, 1.616444807786208, 1.6251682043075562)\n",
      "(epoch, train_loss, val_loss) = (966, 1.6193376687856822, 1.6292470693588257)\n",
      "(epoch, train_loss, val_loss) = (967, 1.6213651803823619, 1.6316590309143066)\n",
      "(epoch, train_loss, val_loss) = (968, 1.626203885445228, 1.632239818572998)\n",
      "(epoch, train_loss, val_loss) = (969, 1.6243320520107563, 1.6309258937835693)\n",
      "(epoch, train_loss, val_loss) = (970, 1.6283839207429152, 1.6278481483459473)\n",
      "(epoch, train_loss, val_loss) = (971, 1.6212650537490845, 1.623246431350708)\n",
      "(epoch, train_loss, val_loss) = (972, 1.6172051888245802, 1.6176143884658813)\n",
      "(epoch, train_loss, val_loss) = (973, 1.6049699691625743, 1.6113852262496948)\n",
      "(epoch, train_loss, val_loss) = (974, 1.6020446924062877, 1.6050808429718018)\n",
      "(epoch, train_loss, val_loss) = (975, 1.6044234312497652, 1.5992240905761719)\n",
      "(epoch, train_loss, val_loss) = (976, 1.5948642308895404, 1.5942350625991821)\n",
      "(epoch, train_loss, val_loss) = (977, 1.5891696948271532, 1.5904995203018188)\n",
      "(epoch, train_loss, val_loss) = (978, 1.5885362808520977, 1.5882627964019775)\n",
      "(epoch, train_loss, val_loss) = (979, 1.5881072649588952, 1.5875850915908813)\n",
      "(epoch, train_loss, val_loss) = (980, 1.589622369179359, 1.588391900062561)\n",
      "(epoch, train_loss, val_loss) = (981, 1.5933943069898164, 1.5904335975646973)\n",
      "(epoch, train_loss, val_loss) = (982, 1.5984722742667565, 1.5934354066848755)\n",
      "(epoch, train_loss, val_loss) = (983, 1.5978414095365083, 1.5970277786254883)\n",
      "(epoch, train_loss, val_loss) = (984, 1.6036427387824426, 1.600880742073059)\n",
      "(epoch, train_loss, val_loss) = (985, 1.6095147683070257, 1.6045265197753906)\n",
      "(epoch, train_loss, val_loss) = (986, 1.6109815927652211, 1.607606291770935)\n",
      "(epoch, train_loss, val_loss) = (987, 1.6157893125827496, 1.6098626852035522)\n",
      "(epoch, train_loss, val_loss) = (988, 1.6161175691164458, 1.611104965209961)\n",
      "(epoch, train_loss, val_loss) = (989, 1.6180647611618042, 1.6112345457077026)\n",
      "(epoch, train_loss, val_loss) = (990, 1.616645941367516, 1.610254168510437)\n",
      "(epoch, train_loss, val_loss) = (991, 1.6141158434060903, 1.6082189083099365)\n",
      "(epoch, train_loss, val_loss) = (992, 1.613827842932481, 1.6053016185760498)\n",
      "(epoch, train_loss, val_loss) = (993, 1.60841350372021, 1.6017875671386719)\n",
      "(epoch, train_loss, val_loss) = (994, 1.6019188532462487, 1.597975254058838)\n",
      "(epoch, train_loss, val_loss) = (995, 1.6026384005179772, 1.5941575765609741)\n",
      "(epoch, train_loss, val_loss) = (996, 1.5965158205765944, 1.5905957221984863)\n",
      "(epoch, train_loss, val_loss) = (997, 1.590667697099539, 1.5875794887542725)\n",
      "(epoch, train_loss, val_loss) = (998, 1.5889038489415095, 1.5853685140609741)\n",
      "(epoch, train_loss, val_loss) = (999, 1.5834742417702308, 1.5841090679168701)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-5\n",
    "HIDDEN_LAYER_SIZES = [32 * 32 * 3, 128, 32, 10]\n",
    "\n",
    "net = NeuralNet(HIDDEN_LAYER_SIZES)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(net)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        images = reshape(images)\n",
    "        output = net(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        \n",
    "        ### <YOUR CODE HERE> ####\n",
    "        # Zero gradients, call .backward(), and step the optimizer.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ### </YOUR CODE HERE> ###\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        \n",
    "    average_loss /= len(trainloader)\n",
    "    \n",
    "    val_output = net(reshape(valset[0]))\n",
    "    val_loss = loss_fn(val_output, valset[1]).item()\n",
    "    \n",
    "    print(\"(epoch, train_loss, val_loss) = ({0}, {1}, {2})\".format(epoch, average_loss, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4332\n"
     ]
    }
   ],
   "source": [
    "### Here, we test the overall accuracy of our model. ###\n",
    "test_output = net(reshape(testset[0]))\n",
    "test_maxes = torch.argmax(test_output, dim=1)\n",
    "print(\"Test accuracy:\", torch.sum(test_maxes == testset[1]).item() / float(test_maxes.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
